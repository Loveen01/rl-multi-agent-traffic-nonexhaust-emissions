{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os \n",
    "\n",
    "import traci\n",
    "import sumo_rl\n",
    "os.environ['SUMO_HOME'] = '/opt/homebrew/opt/sumo/share/sumo'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/loveen/.pyenv/versions/3.9.10/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-01-22 15:25:16,270\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-01-22 15:25:16,391\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menvironment\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreward_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m combined_reward\n\u001b[1;32m     10\u001b[0m env_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/2x2grid\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m multi_agent_env \u001b[38;5;241m=\u001b[39m parallel_env(    \n\u001b[0;32m---> 13\u001b[0m         net_file \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(env_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2x2.net.xml\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     14\u001b[0m         route_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(env_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2x2.rou.xml\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     15\u001b[0m         reward_fn \u001b[38;5;241m=\u001b[39m combined_reward,\n\u001b[1;32m     16\u001b[0m         observation_class \u001b[38;5;241m=\u001b[39m EntireObservationFunction, \n\u001b[1;32m     17\u001b[0m         out_csv_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs/2x2grid/ppo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     18\u001b[0m         num_seconds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m     19\u001b[0m         add_per_agent_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     20\u001b[0m         add_system_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     21\u001b[0m         single_agent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "from sumo_rl.environment.env import env, parallel_env, SumoEnvironment\n",
    "from ray.tune import register_env\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv\n",
    "from ray.rllib.env.wrappers.multi_agent_env_compatibility import MultiAgentEnvCompatibility\n",
    "\n",
    "from environment.envs import RealMultiAgentSumoEnv\n",
    "from environment.observation import Grid2x2ObservationFunction, EntireObservationFunction\n",
    "from environment.reward_functions import combined_reward\n",
    "\n",
    "env_folder = \"data/2x2grid\"\n",
    "\n",
    "multi_agent_env = parallel_env(    \n",
    "        net_file = os.path.join(env_folder, \"2x2.net.xml\"),\n",
    "        route_file = os.path.join(env_folder, \"2x2.rou.xml\"),\n",
    "        reward_fn = combined_reward,\n",
    "        observation_class = EntireObservationFunction, \n",
    "        out_csv_name=\"outputs/2x2grid/ppo\", \n",
    "        num_seconds=1000,\n",
    "        add_per_agent_info=True,\n",
    "        add_system_info=True,\n",
    "        single_agent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '5', '6']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_agent_env.possible_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.soft_max = nn.Softmax()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        self.linear_network = nn.Sequential(\n",
    "            nn.Linear(self.state_dim,100), # 2 - 3 layers. 84 neurones because each policy absorbs the entire observation space \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,100), # 100 neurones to start with - neurones should be approx within range of no. features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,self.action_dim), # 4 represents the phases each intersection entails. \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if type(x)==dict:\n",
    "            arr = np.array(x.values())\n",
    "            x = torch.from_numpy(arr)\n",
    "            print(x)\n",
    "        if type(x)==np.ndarray:\n",
    "            x = torch.from_numpy(arr)\n",
    "        output = self.linear_network(x.float())\n",
    "        logits = self.soft_max(output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.soft_max = nn.Softmax()\n",
    "        self.state_dim = state_dim\n",
    "        \n",
    "        self.linear_network = nn.Sequential(\n",
    "            nn.Linear(self.state_dim,100), # 2 - 3 layers. 84 neurones because each policy absorbs the entire observation space \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,100), # 100 neurones to start with - neurones should be approx within range of no. features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,1), # outputs the value of being in a particular state \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if type(x)==dict:\n",
    "            arr = np.array(x.values())\n",
    "            x = torch.from_numpy(arr)\n",
    "        if type(x)==np.ndarray:\n",
    "            x = torch.from_numpy(x)\n",
    "        output = self.linear_network(x.float())\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_arr_to_tensor(obs:np.ndarray):\n",
    "    return torch.from_numpy(obs)\n",
    "\n",
    "def get_agent_observation_as_tensor(all_agents_obs:dict, agent_id:str):\n",
    "    '''Takes in entire observations, returning observations for particular agent in tensor form'''\n",
    "    agent_obs = all_agents_obs[agent_id]\n",
    "    return convert_arr_to_tensor(agent_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "from copy import deepcopy\n",
    "import logging \n",
    "logging.basicConfig(filename='train_logs.log', level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, multi_agent_par_env, state_dim, action_dim, lr=1e-3, discount=0.99, clip_epsilon=0.2):\n",
    "        self.agents_neuralnetwork = [self._init_agent(state_dim, action_dim, lr) for _ in range(multi_agent_par_env.max_num_agents)]\n",
    "        self.discount = discount\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.multi_agent_env = multi_agent_par_env\n",
    "        # self.entropy_coeff\n",
    "\n",
    "        self.agent_ids = self.multi_agent_env.possible_agents # list \n",
    "        self.num_agents = self.multi_agent_env.max_num_agents\n",
    "        self.state_dim = state_dim #84\n",
    " \n",
    "        self.update_agents_neuralnetwork_old() # dont think you even need this. \n",
    "\n",
    "    def _init_agent(self, state_dim, action_dim, lr):\n",
    "        policy_net = PolicyNetwork(state_dim, action_dim)\n",
    "        value_net = ValueNetwork(state_dim)\n",
    "        policy_optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "        value_optimizer = optim.Adam(value_net.parameters(), lr=lr)\n",
    "        return {\"policy\": policy_net, \"value\": value_net, \"policy_opt\": policy_optimizer, \"value_opt\": value_optimizer}\n",
    "\n",
    "    def sample_policy_action(self, agent_i:int, observations) -> torch.Tensor :\n",
    "        return self.agents_neuralnetwork[agent_i]['policy'](observations)\n",
    "    \n",
    "    def sample_policy_actions(self, observations:dict) -> dict:\n",
    "        '''Takes in observations of all agents, returning the action vectors for use in environment'''\n",
    "        return {id: self.sample_policy_action(i, observations[id]) for i, id in enumerate(self.agent_ids)}\n",
    "\n",
    "    def generate_value(self, agent_i, agent_observations:torch.Tensor) -> torch.Tensor: \n",
    "        value = self.agents_neuralnetwork[agent_i]['value'](agent_observations)\n",
    "        return value.detach().numpy()\n",
    "\n",
    "    def generate_values(self, observations) -> np.ndarray: \n",
    "        ''''Generates values for all agents when given all observations'''\n",
    "        return np.array([self.generate_value(i, agent_observations) for i, agent_observations in enumerate(self.agent_ids, observations)])\n",
    "    \n",
    "    def _generate_trajectory_np(self, observations:dict, no_timesteps:int) -> tuple:\n",
    "        '''Expecting observations for all agents from multi-agent parallel environment setup.\n",
    "        Ensure to pass in the current observations of all agents in env'''\n",
    "\n",
    "        observation_trajectories = np.zeros((self.num_agents, no_timesteps, self.state_dim))\n",
    "        action_trajectories = np.zeros((self.num_agents, no_timesteps))\n",
    "        reward_trajectories = np.zeros((self.num_agents, no_timesteps))\n",
    "        pred_prob_trajectories = np.zeros((self.num_agents, no_timesteps))\n",
    "\n",
    "        for t in range(no_timesteps):\n",
    "\n",
    "            agents_actions = {agent_id:None for agent_id in self.agent_ids} # initialise\n",
    "\n",
    "            print('start of outer loop agents actions: ', agents_actions)\n",
    "\n",
    "            for i, id in enumerate(self.agent_ids):\n",
    "                # get immediate action from policy network\n",
    "                agent_obs = get_agent_observation_as_tensor(observations, agent_id=id)\n",
    "                # print(agent_obs)\n",
    "                pred_probs = self.sample_policy_action(i, agent_obs) # Each agent will sample from its own policy\n",
    "                action = pred_probs.argmax()\n",
    "                print(\"inner loop: \", action)\n",
    "\n",
    "                observation_trajectories[i][t] = agent_obs[t].numpy()\n",
    "                action_trajectories[i][t] = action\n",
    "                pred_prob_trajectories[i][t] = pred_probs.max().detach().numpy()\n",
    "\n",
    "                agents_actions[id]= int(action) # update this, as next it will go in the step() func\n",
    "            \n",
    "            print('end of inner loop agents actions: ', agents_actions)\n",
    "\n",
    "            observations, rewards, terminations, truncations, infos = self.multi_agent_env.step(agents_actions) # takes in a dictionary of all agents + their corresponding actions\n",
    "            \n",
    "            # store rewards recieved after taking step \n",
    "            for i, id in enumerate(self.agent_ids):\n",
    "                reward_trajectories[i][t] = rewards[id]\n",
    "\n",
    "        return observation_trajectories, action_trajectories, pred_prob_trajectories, reward_trajectories\n",
    "\n",
    "    def generate_episodes_np(self, observations, no_episodes, no_timesteps) -> tuple:\n",
    "        '''Generate multiple episodes returning obs, actions, rewards, advantages tensors over all the episodes'''\n",
    "\n",
    "        ep_obs = np.zeros((self.num_agents, no_episodes, no_timesteps, self.state_dim))\n",
    "        ep_actions = np.zeros((self.num_agents, no_episodes, no_timesteps))\n",
    "        ep_rewards = np.zeros((self.num_agents, no_episodes, no_timesteps))\n",
    "\n",
    "        ep_returns = np.zeros((self.num_agents, no_episodes, no_timesteps))\n",
    "        ep_pred_values = np.zeros((self.num_agents, no_episodes, no_timesteps))\n",
    "        ep_advantages = np.zeros((self.num_agents, no_episodes, no_timesteps)) # returns - pred_values\n",
    "\n",
    "        ep_pred_probs = np.zeros((self.num_agents, no_episodes, no_timesteps))\n",
    "\n",
    "        for ep_i in range(no_episodes): \n",
    "            # generate a trajectory over t timesteps \n",
    "            observations_trajec, actions_trajec, pred_prob_trajec, rewards_trajec = self._generate_trajectory_np(observations, no_timesteps)\n",
    "\n",
    "            for agent_i, agent_id in enumerate(self.agent_ids):\n",
    "\n",
    "                reversed_returns =  np.zeros((no_timesteps))\n",
    "                reversed_pred_values = np.zeros((no_timesteps))\n",
    "\n",
    "                reversed_advantages = np.zeros((no_timesteps)) # returns - pred_values(V(s))\n",
    "\n",
    "                running_returns = 0\n",
    "\n",
    "                for t in reversed(range(no_timesteps)): # t is reversed here \n",
    "                    \n",
    "                    # calculate returns from rewards \n",
    "                    rewards = rewards_trajec[agent_i][t]\n",
    "                    running_returns += rewards # we have not implemented a 1 step return, currently it consists of entire return. \n",
    "                    reversed_returns[t] = running_returns \n",
    "\n",
    "                    # calculate predicted values from value network at time t \n",
    "                    agent_observations_t = observations_trajec[agent_i][t]\n",
    "                    agent_pred_value = self.generate_value(agent_i, agent_observations_t) # simple forward pass in network to calculate value of state. \n",
    "\n",
    "                    # calculate advantage at time t \n",
    "                    advantage = running_returns - agent_pred_value\n",
    "\n",
    "                    reversed_advantages[t] = advantage\n",
    "                    reversed_pred_values[t] = agent_pred_value\n",
    "\n",
    "                # reverse all arrays\n",
    "                advantages = reversed_advantages[::-1]\n",
    "                returns = reversed_returns[::-1] \n",
    "                pred_values = reversed_pred_values[::-1]\n",
    "\n",
    "                # append [t] arrays to episodic arrays\n",
    "                ep_advantages[agent_i][ep_i] = advantages \n",
    "                ep_returns[agent_i][ep_i] = returns\n",
    "                ep_pred_values[agent_i][ep_i] = pred_values\n",
    "\n",
    "                # append other data to large episode tensor\n",
    "                ep_obs[agent_i][ep_i] = observations_trajec[agent_i]\n",
    "                ep_actions[agent_i][ep_i] = actions_trajec[agent_i]\n",
    "                ep_rewards[agent_i][ep_i] = rewards_trajec[agent_i]\n",
    "                ep_pred_probs[agent_i][ep_i] = pred_prob_trajec[agent_i]\n",
    "\n",
    "        return ep_obs, ep_actions, ep_pred_probs, ep_rewards, ep_returns, ep_pred_values, ep_advantages\n",
    "\n",
    "    def __compute_policy_loss(self, old_log_probs, new_log_probs, advantages):\n",
    "        '''Takes sequence of log_probs and advantages in tensors, calculates J(0) which is the prob ratios * advantages'''\n",
    "\n",
    "        # Calculate the ratio of new and old probabilities\n",
    "        ratios = torch.exp(new_log_probs - old_log_probs)\n",
    "\n",
    "        # Calculate surrogate loss\n",
    "        surr1 = ratios * advantages\n",
    "        surr2 = torch.clamp(ratios, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()  # Negative because we perform gradient ascent\n",
    "\n",
    "        return policy_loss\n",
    "    \n",
    "    def __compute_value_loss(self, actual_returns, predicted_values) -> torch.Tensor:\n",
    "\n",
    "        # Mean squared error loss between predicted and actual returns\n",
    "        value_loss = F.mse_loss(predicted_values, actual_returns)\n",
    "\n",
    "        return value_loss\n",
    "    \n",
    "    def update_agents_neuralnetwork_old(self):\n",
    "        '''This function does a deep copy of the current agent policies, returning dictionaries'''\n",
    "\n",
    "        self.agents_neuralnetwork_old = [{\"policy\": deepcopy(self.agents_neuralnetwork[i]['policy']), \"value\": deepcopy(self.agents_neuralnetwork[i]['value']), \\\n",
    "                                         \"policy_opt\": deepcopy(self.agents_neuralnetwork[i]['policy_opt']), \\\n",
    "                                            \"value_opt\": deepcopy(self.agents_neuralnetwork[i]['value_opt'])}\\\n",
    "                                         for i in range(self.num_agents)]\n",
    "\n",
    "\n",
    "    def update_network(self, agent_enumer, observations_batch, pred_probs_batch, returns_batch, pred_values_batch, advantages_batch):\n",
    "        '''Expects data in tensor format, returns in tensor format'''\n",
    "    \n",
    "        agent_i, agent_id = agent_enumer # unwrap tuple(int,str)\n",
    "\n",
    "        old_pred_probs_batch = torch.zeros((len(observations_batch)), dtype=float)\n",
    "        \n",
    "        # agent_neuralnetwork_old = self.agents_neuralnetwork_old[agent_i]\n",
    "        agent_neuralnetwork_old = self.agents_neuralnetwork[agent_i]\n",
    "\n",
    "        # calc predicted probability from the old network - do it for [obs1, obs2, obs3, ...]\n",
    "        for i, observations in enumerate(observations_batch):\n",
    "            # print(i, observations)\n",
    "            old_pred_probs_batch[i] = agent_neuralnetwork_old['policy'](observations).max()\n",
    "        \n",
    "        policy_loss = self.__compute_policy_loss(old_pred_probs_batch, pred_probs_batch, advantages_batch)  \n",
    "        \n",
    "        # Backpropagate policy loss\n",
    "        policy_optimiser = self.agents_neuralnetwork[agent_i]['policy_opt']\n",
    "        \n",
    "        policy_optimiser.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        policy_optimiser.step()\n",
    "\n",
    "        # Calculate value loss\n",
    "        value_loss = self.__compute_value_loss(returns_batch, pred_values_batch)\n",
    "\n",
    "        # Backpropagate value loss\n",
    "        value_optimiser = self.agents_neuralnetwork[agent_i]['value_opt']\n",
    "        \n",
    "        value_optimiser.zero_grad()\n",
    "        value_loss.backward()\n",
    "        value_optimiser.step()\n",
    "\n",
    "        return policy_loss, value_loss \n",
    "\n",
    "    def train(self, no_episodes, no_timesteps, minibatch_size, no_epochs):\n",
    "        ''''MA Training happens here. Updates the network, keeping track of metrics throughout'''\n",
    "        logging.debug(\"Entering train()\")\n",
    "        \n",
    "        train_info_index = 0\n",
    "\n",
    "        batch_size = no_episodes * no_timesteps # e.g. 4*100*100 -> 40000 this is due to flattening the vectors.  \n",
    "\n",
    "        sampler_size = batch_size // minibatch_size # from predicting size \n",
    "\n",
    "        total_no_losses = no_epochs * sampler_size\n",
    "\n",
    "        # All training data will go here \n",
    "        train_info = {id:{'policy_loss': np.zeros((total_no_losses)), 'value_loss':np.zeros((total_no_losses))} for id in self.agent_ids}\n",
    "\n",
    "        observations, truncations = self.multi_agent_env.reset()\n",
    "\n",
    "        for epoch_i in range(no_epochs):\n",
    "            \n",
    "            # logging.debug(\"Epoch no\", epoch_i)\n",
    "\n",
    "            ep_obs, ep_actions, ep_pred_probs, ep_rewards, \\\n",
    "                ep_returns, ep_pred_values, ep_advantages = self.generate_episodes_np(observations, no_episodes, no_timesteps)\n",
    "            \n",
    "            # logging.debug(\"Generate_episodes() returns\", 'ep_obs_shape', np.shape(ep_obs), \\\n",
    "            #                                              'ep_actions_shape', np.shape(ep_actions), \\\n",
    "            #                                              'ep_pred_probs_shape',  np.shape(ep_pred_probs), \\\n",
    "            #                                              'ep_rewards_shape',  np.shape(ep_rewards), \\\n",
    "            #                                              'ep_returns', np.shape(ep_returns), \\\n",
    "            #                                               'ep_pred_values_shape', np.shape(ep_pred_values), \\\n",
    "            #                                               'ep_advantages_shape', np.shape(ep_advantages))\n",
    "\n",
    "            logging.debug(\"Epoch no\", epoch_i)\n",
    "            # episodic_obs (no_agents, no_epi, time_steps, dim)\n",
    "\n",
    "            subset_sampler = SubsetRandomSampler(range(batch_size)) # random assort integers from 1 - 84, put in list [3, 4, 9, 84, ...]\n",
    "            sampler = BatchSampler(subset_sampler, minibatch_size, True) # divide this list into batches of size minibatch_size [1, 3, ..], [54, 76, 2..]\n",
    "\n",
    "            for agent_i, agent_id in enumerate(self.agent_ids): # update network for every agent\n",
    "                \n",
    "                logging.debug(\"loop over agent: \", agent_i)\n",
    "\n",
    "                agent_obs = ep_obs[agent_i].reshape(-1,84)\n",
    "\n",
    "                agent_returns = ep_returns[agent_i].reshape(-1) # (no_epi * time_steps) 1dim\n",
    "                agent_pred_values = ep_pred_values[agent_i].reshape(-1)\n",
    "\n",
    "                agent_pred_probs = ep_pred_probs[agent_i].reshape(-1) # (no_epi, time_steps, dim) -> (no_epi*timesteps, dim)\n",
    "                agent_advantages = ep_advantages[agent_i].reshape(-1)\n",
    "\n",
    "                for k, indices in enumerate(sampler):\n",
    "                \n",
    "                    agent_obs_batch = torch.tensor(agent_obs[indices]) # shape = (len(indices), 84)\n",
    "\n",
    "                    agent_pred_probs_batch = torch.tensor(agent_pred_probs[indices], requires_grad=True)\n",
    "                    agent_advantages_batch = torch.tensor(agent_advantages[indices], requires_grad=True)\n",
    "\n",
    "                    agent_returns_batch = torch.tensor(agent_returns[indices], requires_grad=True)\n",
    "                    agent_pred_values_batch = torch.tensor(agent_pred_values[indices], requires_grad=True)\n",
    "\n",
    "                    policy_loss, value_loss = self.update_network((agent_i, agent_id), agent_obs_batch, agent_pred_probs_batch, \\\n",
    "                                                                   agent_returns_batch, agent_pred_values_batch, agent_advantages_batch,)\n",
    "                    \n",
    "                    train_info[agent_id]['policy_loss'][train_info_index+k] = policy_loss\n",
    "                    train_info[agent_id]['value_loss'][train_info_index+k] = value_loss\n",
    "\n",
    "            train_info_index += len(sampler)\n",
    "\n",
    "            # observations = {agent_id:ep_obs[agent_i][-1][-1] for agent_i, agent_id in enumerate(self.agent_ids)}\n",
    "        return train_info   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'multi_agent_env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ppo_algo \u001b[38;5;241m=\u001b[39m PPO(\u001b[43mmulti_agent_env\u001b[49m, \u001b[38;5;241m84\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'multi_agent_env' is not defined"
     ]
    }
   ],
   "source": [
    "ppo_algo = PPO(multi_agent_env, 84, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #1000.00 (0ms ?*RT. ?UPS, TraCI: 34934ms, vehicles TOT 167 ACT 60 BUF 249)            \n",
      " Retrying in 1 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 00:00:57,649 - DEBUG - Entering train()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #0.00 (0ms ?*RT. ?UPS, TraCI: 54ms, vehicles TOT 0 ACT 0 BUF 0)                     \n",
      " Retrying in 1 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/loveen/.pyenv/versions/3.9.10/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/Users/loveen/.pyenv/versions/3.9.10/lib/python3.9/logging/__init__.py\", line 927, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/Users/loveen/.pyenv/versions/3.9.10/lib/python3.9/logging/__init__.py\", line 663, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"/Users/loveen/.pyenv/versions/3.9.10/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/Users/loveen/.pyenv/versions/3.9.10/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Users/loveen/.pyenv/versions/3.9.10/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/loveen/.pyenv/versions/3.9.10/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/loveen/.pyenv/versions/3.9.10/lib/python3.9/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/loveen/.pyenv/versions/3.9.10/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 725, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/loveen/.pyenv/versions/3.9.10/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 215, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/loveen/.pyenv/versions/3.9.10/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/loveen/.pyenv/versions/3.9.10/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/loveen/.pyenv/versions/3.9.10/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/loveen/.pyenv/versions/3.9.10/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/loveen/.pyenv/versions/3.9.10/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/loveen/.pyenv/versions/3.9.10/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/loveen/.pyenv/versions/3.9.10/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/loveen/.pyenv/versions/3.9.10/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/loveen/.pyenv/versions/3.9.10/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/loveen/.pyenv/versions/3.9.10/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/loveen/.pyenv/versions/3.9.10/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3016, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/loveen/.pyenv/versions/3.9.10/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/loveen/.pyenv/versions/3.9.10/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3221, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/loveen/.pyenv/versions/3.9.10/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/loveen/.pyenv/versions/3.9.10/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/nq/yzy9w2_93tj27l52hnwz3knm0000gn/T/ipykernel_3765/3891013356.py\", line 2, in <module>\n",
      "    train_info = ppo_algo.train(50, 30, 10, 1)\n",
      "  File \"/var/folders/nq/yzy9w2_93tj27l52hnwz3knm0000gn/T/ipykernel_3765/2714288532.py\", line 230, in train\n",
      "    logging.debug(\"Epoch no\", epoch_i)\n",
      "Message: 'Epoch no'\n",
      "Arguments: (0,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "end of inner loop agents actions:  {'1': 2, '2': 2, '5': 0, '6': 3}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[134], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m multi_agent_env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m----> 2\u001b[0m train_info \u001b[38;5;241m=\u001b[39m \u001b[43mppo_algo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[130], line 233\u001b[0m, in \u001b[0;36mPPO.train\u001b[0;34m(self, no_episodes, no_timesteps, minibatch_size, no_epochs)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(no_epochs):\n\u001b[1;32m    230\u001b[0m     logging\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch no\u001b[39m\u001b[38;5;124m\"\u001b[39m, epoch_i)\n\u001b[1;32m    232\u001b[0m     ep_obs, ep_actions, ep_pred_probs, ep_rewards, \\\n\u001b[0;32m--> 233\u001b[0m         ep_returns, ep_pred_values, ep_advantages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_episodes_np\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_timesteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# logging.debug(\"Generate_episodes() returns\", 'ep_obs_shape', np.shape(ep_obs), \\\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;66;03m#                                              'ep_actions_shape', np.shape(ep_actions), \\\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m#                                              'ep_pred_probs_shape',  np.shape(ep_pred_probs), \\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m#                                               'ep_pred_values_shape', np.shape(ep_pred_values), \\\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m#                                               'ep_advantages_shape', np.shape(ep_advantages))\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     logging\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch no\u001b[39m\u001b[38;5;124m\"\u001b[39m, epoch_i)\n",
      "Cell \u001b[0;32mIn[130], line 100\u001b[0m, in \u001b[0;36mPPO.generate_episodes_np\u001b[0;34m(self, observations, no_episodes, no_timesteps)\u001b[0m\n\u001b[1;32m     96\u001b[0m ep_pred_probs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_agents, no_episodes, no_timesteps))\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(no_episodes): \n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# generate a trajectory over t timesteps \u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m     observations_trajec, actions_trajec, pred_prob_trajec, rewards_trajec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_trajectory_np\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_timesteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m agent_i, agent_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_ids):\n\u001b[1;32m    104\u001b[0m         reversed_returns \u001b[38;5;241m=\u001b[39m  np\u001b[38;5;241m.\u001b[39mzeros((no_timesteps))\n",
      "Cell \u001b[0;32mIn[130], line 63\u001b[0m, in \u001b[0;36mPPO._generate_trajectory_np\u001b[0;34m(self, observations, no_timesteps)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart of outer loop agents actions: \u001b[39m\u001b[38;5;124m'\u001b[39m, agents_actions)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_ids):\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# get immediate action from policy network\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m     agent_obs \u001b[38;5;241m=\u001b[39m \u001b[43mget_agent_observation_as_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# print(agent_obs)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     pred_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_policy_action(i, agent_obs) \u001b[38;5;66;03m# Each agent will sample from its own policy\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[123], line 6\u001b[0m, in \u001b[0;36mget_agent_observation_as_tensor\u001b[0;34m(all_agents_obs, agent_id)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_agent_observation_as_tensor\u001b[39m(all_agents_obs:\u001b[38;5;28mdict\u001b[39m, agent_id:\u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m      5\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Takes in entire observations, returning observations for particular agent in tensor form'''\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     agent_obs \u001b[38;5;241m=\u001b[39m \u001b[43mall_agents_obs\u001b[49m\u001b[43m[\u001b[49m\u001b[43magent_id\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_arr_to_tensor(agent_obs)\n",
      "\u001b[0;31mKeyError\u001b[0m: '1'"
     ]
    }
   ],
   "source": [
    "multi_agent_env.reset()\n",
    "train_info = ppo_algo.train(50, 30, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_loss = [train_info[id]['policy_loss'] for id in multi_agent_env.possible_agents]\n",
    "value_loss = [train_info[id]['value_loss'] for id in multi_agent_env.possible_agents]\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(policy_loss[0])\n",
    "# plt.legend([1,2,3,4])\n",
    "plt.show()\n",
    "\n",
    "len(policy_loss[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Retrying in 1 seconds\n",
      "Step #0.00 (0ms ?*RT. ?UPS, TraCI: 11ms, vehicles TOT 0 ACT 0 BUF 0)                     \n",
      " Retrying in 1 seconds\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(1)\n",
      "inner loop:  tensor(3)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(1)\n",
      "end of inner loop agents actions:  {'1': 1, '2': 3, '5': 2, '6': 1}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(1)\n",
      "inner loop:  tensor(3)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(1)\n",
      "end of inner loop agents actions:  {'1': 1, '2': 3, '5': 2, '6': 1}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(1)\n",
      "inner loop:  tensor(3)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(1)\n",
      "end of inner loop agents actions:  {'1': 1, '2': 3, '5': 2, '6': 1}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(0)\n",
      "inner loop:  tensor(3)\n",
      "inner loop:  tensor(2)\n",
      "inner loop:  tensor(1)\n",
      "end of inner loop agents actions:  {'1': 0, '2': 3, '5': 2, '6': 1}\n",
      "start of outer loop agents actions:  {'1': None, '2': None, '5': None, '6': None}\n",
      "inner loop:  tensor(1)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for dimension 0 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m multi_agent_env_2 \u001b[38;5;241m=\u001b[39m parallel_env(    \n\u001b[1;32m      2\u001b[0m         net_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(env_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2x2.net.xml\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      3\u001b[0m         route_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(env_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2x2.rou.xml\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m         add_system_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m         single_agent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m ppo_algo \u001b[38;5;241m=\u001b[39m PPO(multi_agent_env_2, \u001b[38;5;241m84\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m train_info \u001b[38;5;241m=\u001b[39m \u001b[43mppo_algo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 228\u001b[0m, in \u001b[0;36mPPO.train\u001b[0;34m(self, minibatch_size, no_episodes, no_timesteps, no_epochs)\u001b[0m\n\u001b[1;32m    223\u001b[0m observations, truncations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_agent_env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(no_epochs):\n\u001b[1;32m    227\u001b[0m     ep_obs, ep_actions, ep_pred_probs, ep_rewards, \\\n\u001b[0;32m--> 228\u001b[0m         ep_returns, ep_pred_values, ep_advantages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_episodes_np\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_timesteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m# episodic_obs (no_agents, no_epi, time_steps, dim)\u001b[39;00m\n\u001b[1;32m    232\u001b[0m     subset_sampler \u001b[38;5;241m=\u001b[39m SubsetRandomSampler(\u001b[38;5;28mrange\u001b[39m(batch_size)) \u001b[38;5;66;03m# random assort integers from 1 - 84, put in list [3, 4, 9, 84, ...]\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[29], line 99\u001b[0m, in \u001b[0;36mPPO.generate_episodes_np\u001b[0;34m(self, observations, no_episodes, no_timesteps)\u001b[0m\n\u001b[1;32m     95\u001b[0m ep_pred_probs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_agents, no_episodes, no_timesteps))\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(no_episodes): \n\u001b[1;32m     98\u001b[0m     \u001b[38;5;66;03m# generate a trajectory over t timesteps \u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m     observations_trajec, actions_trajec, pred_prob_trajec, rewards_trajec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_trajectory_np\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_timesteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m agent_i, agent_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_ids):\n\u001b[1;32m    103\u001b[0m         reversed_returns \u001b[38;5;241m=\u001b[39m  np\u001b[38;5;241m.\u001b[39mzeros((no_timesteps))\n",
      "Cell \u001b[0;32mIn[29], line 70\u001b[0m, in \u001b[0;36mPPO._generate_trajectory_np\u001b[0;34m(self, observations, no_timesteps)\u001b[0m\n\u001b[1;32m     68\u001b[0m     observation_trajectories[i][t] \u001b[38;5;241m=\u001b[39m agent_obs[t]\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     69\u001b[0m     action_trajectories[i][t] \u001b[38;5;241m=\u001b[39m action\n\u001b[0;32m---> 70\u001b[0m     pred_prob_trajectories[i][t] \u001b[38;5;241m=\u001b[39m \u001b[43mpred_probs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     72\u001b[0m     agents_actions[\u001b[38;5;28mid\u001b[39m]\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(action) \u001b[38;5;66;03m# update this, as next it will go in the step() func\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend of inner loop agents actions: \u001b[39m\u001b[38;5;124m'\u001b[39m, agents_actions)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 4 is out of bounds for dimension 0 with size 4"
     ]
    }
   ],
   "source": [
    "multi_agent_env_2 = parallel_env(    \n",
    "        net_file = os.path.join(env_folder, \"2x2.net.xml\"),\n",
    "        route_file = os.path.join(env_folder, \"2x2.rou.xml\"),\n",
    "        reward_fn = combined_reward,\n",
    "        observation_class = EntireObservationFunction, \n",
    "        out_csv_name=\"outputs/2x2grid/ppo\", \n",
    "        num_seconds=1000,\n",
    "        add_per_agent_info=True,\n",
    "        add_system_info=True,\n",
    "        single_agent=False)\n",
    "\n",
    "ppo_algo = PPO(multi_agent_env_2, 84, 4)\n",
    "train_info = ppo_algo.train(4, 10, 15, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agents_neuralnetwork_old' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m agents_neuralnetwork \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m8\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m ppo_algo \u001b[38;5;241m=\u001b[39m \u001b[43mPPO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmulti_agent_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m84\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deepcopy\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# h = {id: action for i, (id, action) in enumerate(zip(['1', '4', '6', '7'], agents_neuralnetwork[i]))}\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[63], line 20\u001b[0m, in \u001b[0;36mPPO.__init__\u001b[0;34m(self, multi_agent_par_env, state_dim, action_dim, lr, discount, clip_epsilon)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_agents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_agent_env\u001b[38;5;241m.\u001b[39mmax_num_agents\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_dim \u001b[38;5;241m=\u001b[39m state_dim \u001b[38;5;66;03m#84\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[43magents_neuralnetwork_old\u001b[49m): \n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_agents_neuralnetwork_old()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'agents_neuralnetwork_old' is not defined"
     ]
    }
   ],
   "source": [
    "agents_neuralnetwork = [2, 5, 6, 8]\n",
    "\n",
    "ppo_algo = PPO(multi_agent_env, 84, 4)\n",
    "\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "# h = {id: action for i, (id, action) in enumerate(zip(['1', '4', '6', '7'], agents_neuralnetwork[i]))}\n",
    "\n",
    "a = deepcopy(ppo_algo)\n",
    "\n",
    "print(ppo_algo, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_pred_probs_batch = torch.zeros((10))\n",
    "[0]\n",
    "old_pred_probs_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.2000)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surr_1 = torch.tensor([0.5,0.9,0.8,0.6]) \n",
    "surr_2 = torch.tensor([0.7,0.8,0.4,0.5])\n",
    "\n",
    "policy_action_loss = -torch.sum(torch.min(surr_1, surr_2), dim=-1, keepdim=True).mean()\n",
    "policy_action_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.5500)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_loss = -torch.min(surr_1, surr_2).mean()  # Negative because we perform gradient ascent\n",
    "policy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.4000)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.sum(torch.min(surr_1, surr_1), dim=-1, keepdim=True).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((4, 5, 10))\n",
    "b = np.ones((10))\n",
    "\n",
    "a[1][1] = b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[1][1] = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Retrying in 1 seconds\n"
     ]
    }
   ],
   "source": [
    "observations, truncations = multi_agent_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_agent_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {'1':2, '4':5}\n",
    "a['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
