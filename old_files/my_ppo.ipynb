{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os \n",
    "\n",
    "import traci\n",
    "import sumo_rl\n",
    "os.environ['SUMO_HOME'] = '/opt/homebrew/opt/sumo/share/sumo'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Retrying in 1 seconds\n",
      "Step #0.00 (0ms ?*RT. ?UPS, TraCI: 14ms, vehicles TOT 0 ACT 0 BUF 0)                     \n",
      " Retrying in 1 seconds\n"
     ]
    }
   ],
   "source": [
    "from sumo_rl.environment.env import env, parallel_env, SumoEnvironment\n",
    "from ray.tune import register_env\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv\n",
    "from ray.rllib.env.wrappers.multi_agent_env_compatibility import MultiAgentEnvCompatibility\n",
    "\n",
    "from environment.envs import RealMultiAgentSumoEnv\n",
    "from environment.observation import Grid2x2ObservationFunction, EntireObservationFunction\n",
    "from environment.reward_functions import combined_reward\n",
    "\n",
    "env_folder = \"data/2x2grid\"\n",
    "\n",
    "multi_agent_env = RealMultiAgentSumoEnv(    \n",
    "        net_file = os.path.join(env_folder, \"2x2.net.xml\"),\n",
    "        route_file = os.path.join(env_folder, \"2x2.rou.xml\"),\n",
    "        reward_fn = combined_reward,\n",
    "        observation_class = EntireObservationFunction, \n",
    "        out_csv_name=\"outputs/2x2grid/ppo\", \n",
    "        num_seconds=1000,\n",
    "        add_per_agent_info=True,\n",
    "        add_system_info=True,\n",
    "        single_agent=False)\n",
    "\n",
    "rllib_parallel_petting_env = ParallelPettingZooEnv(multi_agent_env)   # ParallelPettingZoo is a wrapper from rrlib, \n",
    "                                                                # that wraps an env into rrlib compatible one, it simplifies the API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_env(name='hjkh', env_creator= lambda config : rllib_parallel_petting_env)\n",
    "import ray\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rllib_parallel_petting_env.get_agent_ids()\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0.0, 1.0, (84,), float32)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_agent_env.unwrapped.observation_space('1')\n",
    "multi_agent_env.unwrapped.env.observation_spaces('2')\n",
    "# multi_agent_env.unwrapped.env.traffic_signals['2'].num_green_phases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch \n",
    "\n",
    "# inputs should be observation_function - size is 84 size array \n",
    "# because each intersection has 8 main lanes, 8 * 2 + (1 + 4) = 21. 4 ts so we have 84 ... \n",
    "# 4 stands for a one-hot encoder for the current traffic phase - each item in it needs to be a separate feature to be injected into the ANN\n",
    "# Use Softmax activation function to chose between the 4 actions  \n",
    "# We want every single agent in network to train its own model - each learns its own policy. \n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.soft_max = nn.Softmax()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        self.linear_network = nn.Sequential(\n",
    "            nn.Linear(self.state_dim,100), # 2 - 3 layers. 84 neurones because each policy absorbs the entire observation space \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,100), # 100 neurones to start with - neurones should be approx within range of no. features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,self.action_dim), # 4 represents the phases each intersection entails. \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if type(x)==dict:\n",
    "            arr = np.array(x.values())\n",
    "            x = torch.from_numpy(arr)   \n",
    "            print(x)\n",
    "        output = self.linear_network(x)\n",
    "        logits = self.soft_max(output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.soft_max = nn.Softmax()\n",
    "        self.state_dim = state_dim\n",
    "        \n",
    "        self.linear_network = nn.Sequential(\n",
    "            nn.Linear(self.state_dim,100), # 2 - 3 layers. 84 neurones because each policy absorbs the entire observation space \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,100), # 100 neurones to start with - neurones should be approx within range of no. features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,1), # outputs the value of being in a particular state \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if type(x)==dict:\n",
    "            arr = np.array(x.values())\n",
    "            x = torch.from_numpy(arr)   \n",
    "            print(x)\n",
    "        output = self.linear_network(x)\n",
    "        return output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Phase(duration=60, state='GGrrrrGGrrrr', minDur=-1, maxDur=-1),\n",
       " Phase(duration=60, state='rrGrrrrrGrrr', minDur=-1, maxDur=-1),\n",
       " Phase(duration=60, state='rrrGGrrrrGGr', minDur=-1, maxDur=-1),\n",
       " Phase(duration=60, state='rrrrrGrrrrrG', minDur=-1, maxDur=-1)]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_agent_env.aec_env.unwrapped.env.traffic_signals['1'].green_phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_action_from_policy(policy_network:PolicyNetwork, obs:torch.Tensor):\n",
    "    ''''Samples action from env, after 1 pass from policy network, using argmax to pick'''\n",
    "    pred_prob = policy_network(obs) # forward propagate the network \n",
    "    return pred_prob.argmax()\n",
    "\n",
    "def convert_arr_to_tensor(obs:np.ndarray):\n",
    "    return torch.from_numpy(obs)\n",
    "\n",
    "def get_agent_observation_as_tensor(all_agents_obs:dict, agent_id:str):\n",
    "    '''Takes in entire observations, returning observations for particular agent in tensor form'''\n",
    "    agent_obs = all_agents_obs[agent_id]\n",
    "    return convert_arr_to_tensor(agent_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_ids = multi_agent_env.possible_agents\n",
    "\n",
    "def instantiate_policies(agent_ids):\n",
    "    policies = {id:PolicyNetwork() for id in agent_ids}\n",
    "    return policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_trajectory(multi_agent_env, all_observations:dict, timesteps:int) -> tuple: \n",
    "    '''Expecting observations for all agents from multi-agent environment setup.\n",
    "    Ensure to pass in the current observations of agents in env'''\n",
    "    \n",
    "    observations = all_observations\n",
    "\n",
    "    num_agents = multi_agent_env.num_agents\n",
    "\n",
    "    observation_trajectories = torch.zeros(num_agents, timesteps, len(observations['1'])) \n",
    "    action_trajectories = torch.zeros(num_agents, timesteps)\n",
    "    reward_trajectories = torch.zeros(num_agents, timesteps)\n",
    "\n",
    "    for t in range(timesteps):\n",
    "\n",
    "        agents_actions = {agent_id:None for agent_id in agent_ids}\n",
    "\n",
    "        for i, id in enumerate(agent_ids):\n",
    "            # get immediate action from policy network\n",
    "            agent_obs = get_agent_observation_as_tensor(observations, agent_id=id)\n",
    "            action = sample_action_from_policy(agent_policies[id], agent_obs) \n",
    "\n",
    "            observation_trajectories[i][t] = agent_obs\n",
    "            action_trajectories[i][t] = action\n",
    "\n",
    "            agents_actions[id]= int(action) # update this, as next it will go in the step() func\n",
    "\n",
    "        # print(agents_actions)\n",
    "\n",
    "        observations, rewards, terminations, truncations, infos = multi_agent_env.step(agents_actions) # takes in a dictionary of all agents + their corresponding actions\n",
    "        \n",
    "        for i, id in enumerate(agent_ids):\n",
    "            reward_trajectories[i][t] = rewards[id]\n",
    "\n",
    "    return observation_trajectories, action_trajectories, reward_trajectories\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Retrying in 1 seconds\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'agent_policies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[154], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m observations, infos \u001b[38;5;241m=\u001b[39m multi_agent_env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m----> 2\u001b[0m o, a, r \u001b[38;5;241m=\u001b[39m \u001b[43m_generate_trajectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmulti_agent_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m observations, infos \u001b[38;5;241m=\u001b[39m multi_agent_env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m      5\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \n",
      "Cell \u001b[0;32mIn[153], line 20\u001b[0m, in \u001b[0;36m_generate_trajectory\u001b[0;34m(multi_agent_env, all_observations, timesteps)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(agent_ids):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# get immediate action from policy network\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     agent_obs \u001b[38;5;241m=\u001b[39m get_agent_observation_as_tensor(observations, agent_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mid\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m     action \u001b[38;5;241m=\u001b[39m sample_action_from_policy(\u001b[43magent_policies\u001b[49m[\u001b[38;5;28mid\u001b[39m], agent_obs) \n\u001b[1;32m     22\u001b[0m     observation_trajectories[i][t] \u001b[38;5;241m=\u001b[39m agent_obs\n\u001b[1;32m     23\u001b[0m     action_trajectories[i][t] \u001b[38;5;241m=\u001b[39m action\n",
      "\u001b[0;31mNameError\u001b[0m: name 'agent_policies' is not defined"
     ]
    }
   ],
   "source": [
    "observations, infos = multi_agent_env.reset()\n",
    "o, a, r = _generate_trajectory(multi_agent_env, observations, 10)\n",
    "\n",
    "observations, infos = multi_agent_env.reset()\n",
    "rewards = False \n",
    "\n",
    "agent_ids = multi_agent_env.possible_agents\n",
    "agent_policies = instantiate_policies(agent_ids)\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "print(np.shape(o))\n",
    "print(np.shape(a))\n",
    "print(np.shape(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def generate_episodes(multi_agent_env_fn, no_episodes, no_timesteps, value_networks: list[ValueNetwork]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    '''Generate multiple episodes returning obs, actions, rewards, advantages tensors over all the episodes'''\n",
    "\n",
    "    num_agents = multi_agent_env_fn.num_agents\n",
    "\n",
    "    # initialise advantage trajectories\n",
    "    episodic_advantages_trajectories = torch.zeros(no_episodes, num_agents, no_timesteps)\n",
    "    episodic_obs_trajectories = torch.zeros(no_episodes, num_agents, no_timesteps, 84)\n",
    "    episodic_reward_trajectories = torch.zeros(no_episodes, num_agents, no_timesteps)\n",
    "    episodic_action_trajectories = torch.zeros(no_episodes, num_agents, no_timesteps)\n",
    "\n",
    "    for ep_i in range(no_episodes): \n",
    "        observations_trajec, actions_trajec, rewards_trajec = _generate_trajectory(multi_agent_env, observations, no_timesteps)\n",
    "\n",
    "        for agent_i, agent_id in enumerate(multi_agent_env.agents):\n",
    "            \n",
    "            reversed_returns = np.zeros((no_timesteps))\n",
    "            reversed_value_baseline =  np.zeros((no_timesteps)) # do we need separate vectors for returns or baseline, or keep just one vector\n",
    "\n",
    "            reversed_advantages = np.zeros((no_timesteps))\n",
    "\n",
    "            running_returns = 0\n",
    "\n",
    "            for t in reversed(range(no_timesteps)): # be careful - t is reversed here \n",
    "                \n",
    "                returns = rewards_trajec[agent_i][t]\n",
    "                running_returns += returns # we have not implemented a 1 step return, currently it consists of entire return. \n",
    "\n",
    "                agent_observations_t = observations_trajec[agent_i][t]\n",
    "                agent_value_func_t = value_networks[agent_i](agent_observations_t) # simple forward pass in network to calculate value of state. \n",
    "\n",
    "                advantage = running_returns - agent_value_func_t\n",
    "\n",
    "                reversed_advantages[t] = advantage\n",
    "\n",
    "            advantages = reversed_advantages[::-1]\n",
    "\n",
    "            for t in range(no_timesteps): \n",
    "                episodic_advantages_trajectories[ep_i][agent_i][t] = advantages[t] # reverse the array\n",
    "            \n",
    "        episodic_obs_trajectories[ep_i] = observations_trajec\n",
    "        episodic_action_trajectories[ep_i] = actions_trajec\n",
    "        episodic_reward_trajectories[ep_i] = rewards_trajec\n",
    "    \n",
    "    return episodic_obs_trajectories, episodic_action_trajectories, episodic_reward_trajectories, episodic_advantages_trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #50.00 (0ms ?*RT. ?UPS, TraCI: 265ms, vehicles TOT 16 ACT 15 BUF 0)                  \n",
      " Retrying in 1 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/loveen/.pyenv/versions/3.9.10/lib/python3.9/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "num_agents = multi_agent_env.num_agents\n",
    "\n",
    "value_networks = [ValueNetwork() for agent in range(num_agents)]\n",
    "\n",
    "observations, info = multi_agent_env.reset()\n",
    "\n",
    "episodic_obs, episodic_action, episodic_reward, episodic_advantages = generate_episodes(multi_agent_env, 5, 10, value_networks) \n",
    "\n",
    "# create batches for agent 0\n",
    "agent_obs_ep0 = episodic_obs[0][0] # outputs [10, 84]\n",
    "agent_obs_ep1 = episodic_obs[1][0] \n",
    "agent_obs_ep2 = episodic_obs[2][0] \n",
    "agent_obs_ep3 = episodic_obs[3][0]  \n",
    "\n",
    "batched_agent_obs = episodic_obs[2][0][0:5] # outputs obs between 0-5 timesteps \n",
    "batched_agent_obs = episodic_obs[3][0][0:5] # outputs obs between 0-5 timesteps \n",
    "\n",
    "agent_0_dataset = TensorDataset(episodic_obs[:][0], episodic_action[:][0], episodic_reward[:][0], episodic_advantages[:][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "from copy import deepcopy\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, multi_agent_par_env, state_dim, action_dim, lr=1e-3, discount=0.99, clip_epsilon=0.2):\n",
    "        self.agents_neuralnetwork = [self._init_agent(state_dim, action_dim, lr) for _ in range(multi_agent_par_env.num_agents)]\n",
    "        self.discount = discount\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.multi_agent_env = multi_agent_par_env\n",
    "        # self.entropy_coeff\n",
    "\n",
    "        self.agent_ids = self.multi_agent_env.possible_agents\n",
    "        self.num_agents = self.multi_agent_env.num_agents\n",
    "\n",
    "        # copy the neural network \n",
    "        # self.agents_neuralnetwork_old = [{\"policy\": self.agents_neuralnetwork[i]['policy'], \"value\": self.agents_neuralnetwork[i]['value'], \\\n",
    "        #                                  \"policy_opt\": self.agents_neuralnetwork[i]['policy_opt'], \"value_opt\": self.agents_neuralnetwork[i]['value_opt']}\\\n",
    "        #                                  for i in range(self.num_agents)]\n",
    "\n",
    "    def _init_agent(self, state_dim, action_dim, lr):\n",
    "        policy_net = PolicyNetwork(state_dim, action_dim)\n",
    "        value_net = ValueNetwork(state_dim)\n",
    "        policy_optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "        value_optimizer = optim.Adam(value_net.parameters(), lr=lr)\n",
    "        return {\"policy\": policy_net, \"value\": value_net, \"policy_opt\": policy_optimizer, \"value_opt\": value_optimizer}\n",
    "\n",
    "    def generate_actions(self, observations:dict):\n",
    "        '''Takes in observations of all agents, returning the action vectors for use in environment'''\n",
    "        return {id: action for i, (id, action) in enumerate(zip(self.agent_ids, self.agents_neuralnetwork[i]['policy'](observations)))}\n",
    "\n",
    "    def sample_policy_action(self, agent_id:int, observations):\n",
    "        return self.agents_neuralnetwork[agent_id]['policy'](observations)\n",
    "\n",
    "    def generate_value(self, agent_id, agent_observations:torch.Tensor) -> torch.Tensor: \n",
    "        return self.agents_neuralnetwork['value'](agent_observations)\n",
    "\n",
    "    def generate_values(self, observations:torch.Tensor): \n",
    "        ''''Generates values for all agents when given all observations'''\n",
    "        return np.array([self.generate_values(id, agent_observations) for id, agent_observations in enumerate(self.agent_ids, observations)])\n",
    "\n",
    "    def compute_advantage(self, agentID): \n",
    "        pass \n",
    "\n",
    "    # def _generate_trajectory(self, observations:dict, timesteps:int) -> tuple:\n",
    "    #     '''Expecting observations for all agents from multi-agent parallel environment setup.\n",
    "    #     Ensure to pass in the current observations of all agents in env'''\n",
    "\n",
    "    #     observation_trajectories = torch.zeros(self.num_agents, timesteps, len(observations['1'])) \n",
    "    #     action_trajectories = torch.zeros(self.num_agents, timesteps)\n",
    "    #     reward_trajectories = torch.zeros(self.num_agents, timesteps)\n",
    "    #     pred_prob_trajectories = torch.zeros(self.num_agents, timesteps)\n",
    "\n",
    "    #     for t in range(timesteps):\n",
    "\n",
    "    #         agents_actions = {agent_id:None for agent_id in self.agent_ids} # initialise\n",
    "\n",
    "    #         for i, id in enumerate(self.agent_ids):\n",
    "    #             # get immediate action from policy network\n",
    "    #             agent_obs = get_agent_observation_as_tensor(observations, agent_id=id)\n",
    "    #             pred_probs = self.sample_policy_action(id, agent_obs) # Each agent will sample from its own policy\n",
    "    #             action = pred_probs.arg_max\n",
    "\n",
    "    #             observation_trajectories[i][t] = agent_obs\n",
    "    #             action_trajectories[i][t] = action\n",
    "    #             pred_prob_trajectories[i][t] = pred_probs.max()\n",
    "\n",
    "    #             agents_actions[id]= int(action) # update this, as next it will go in the step() func\n",
    "\n",
    "    #         observations, rewards, terminations, truncations, infos = self.multi_agent_par_env.step(agents_actions) # takes in a dictionary of all agents + their corresponding actions\n",
    "            \n",
    "    #         for i, id in enumerate(self.agent_ids):\n",
    "    #             reward_trajectories[i][t] = rewards[id]\n",
    "\n",
    "    #     return observation_trajectories, action_trajectories, reward_trajectories, pred_prob_trajectories\n",
    "    \n",
    "    def _generate_trajectory_np(self, observations:dict, timesteps:int) -> tuple:\n",
    "        '''Expecting observations for all agents from multi-agent parallel environment setup.\n",
    "        Ensure to pass in the current observations of all agents in env'''\n",
    "\n",
    "        observation_trajectories = np.zeros((self.num_agents, timesteps, len(observations['1'])))\n",
    "        action_trajectories = np.zeros((self.num_agents, timesteps))\n",
    "        reward_trajectories = np.zeros((self.num_agents, timesteps))\n",
    "        pred_prob_trajectories = np.zeros((self.num_agents, timesteps))\n",
    "\n",
    "        for t in range(timesteps):\n",
    "\n",
    "            agents_actions = {agent_id:None for agent_id in self.agent_ids} # initialise\n",
    "\n",
    "            for i, id in enumerate(self.agent_ids):\n",
    "                # get immediate action from policy network\n",
    "                agent_obs = get_agent_observation_as_tensor(observations, agent_id=id)\n",
    "                pred_probs = self.sample_policy_action(id, agent_obs) # Each agent will sample from its own policy\n",
    "                action = pred_probs.arg_max()\n",
    "\n",
    "                observation_trajectories[i][t] = agent_obs.numpy()\n",
    "                action_trajectories[i][t] = action.numpy()\n",
    "                pred_prob_trajectories[i][t] = pred_probs.max().numpy()\n",
    "\n",
    "                agents_actions[id]= int(action) # update this, as next it will go in the step() func\n",
    "\n",
    "            observations, rewards, terminations, truncations, infos = self.multi_agent_par_env.step(agents_actions) # takes in a dictionary of all agents + their corresponding actions\n",
    "            \n",
    "            for i, id in enumerate(self.agent_ids):\n",
    "                reward_trajectories[i][t] = rewards[id]\n",
    "\n",
    "        return observation_trajectories, action_trajectories, reward_trajectories, pred_prob_trajectories\n",
    "\n",
    "    # def generate_episodes(self, observations, no_episodes, no_timesteps) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    #     '''Generate multiple episodes returning obs, actions, rewards, advantages tensors over all the episodes'''\n",
    "\n",
    "    #     episodic_advantages = torch.zeros(no_episodes, self.num_agents, no_timesteps)\n",
    "    #     episodic_obs = torch.zeros(no_episodes, self.num_agents, no_timesteps, 84)\n",
    "    #     episodic_rewards = torch.zeros(no_episodes, self.num_agents, no_timesteps)\n",
    "    #     episodic_actions = torch.zeros(no_episodes, self.num_agents, no_timesteps)\n",
    "\n",
    "    #     episodic_returns = torch.zeros(no_episodes, self.num_agents, no_timesteps)\n",
    "    #     episodic_pred_probs = torch.zeros(no_episodes, self.num_agents, no_timesteps)\n",
    "    #     episodic_pred_values = torch.zeros(no_episodes, self.num_agents, no_timesteps)\n",
    "\n",
    "\n",
    "    #     for ep_i in range(no_episodes): \n",
    "    #         observations_trajec, actions_trajec, rewards_trajec, pred_prob_trajec = self._generate_trajectory(observations, no_timesteps)\n",
    "\n",
    "    #         for i, agent_id in enumerate(self.agent_ids):\n",
    "                \n",
    "    #             reversed_advantages = np.zeros((no_timesteps))\n",
    "    #             reversed_returns =  np.zeros((no_timesteps))\n",
    "    #             reversed_pred_values = np.zeros((no_timesteps))\n",
    "\n",
    "    #             running_returns = 0\n",
    "\n",
    "    #             for t in reversed(range(no_timesteps)): # t is reversed here \n",
    "                    \n",
    "    #                 # calculate returns from rewards \n",
    "    #                 rewards = rewards_trajec[i][t]\n",
    "    #                 running_returns += rewards # we have not implemented a 1 step return, currently it consists of entire return. \n",
    "    #                 reversed_returns[t] = running_returns \n",
    "\n",
    "    #                 agent_observations_t = observations_trajec[i][t]\n",
    "    #                 agent_pred_value = self.generate_value(agent_id, agent_observations_t) # simple forward pass in network to calculate value of state. \n",
    "\n",
    "    #                 advantage = running_returns - agent_pred_value\n",
    "\n",
    "    #                 reversed_advantages[t] = advantage\n",
    "    #                 reversed_pred_values[t] = agent_pred_value\n",
    "\n",
    "\n",
    "    #             advantages = reversed_advantages[::-1]\n",
    "    #             returns = reversed_returns[::-1] \n",
    "    #             pred_values = reversed_pred_values[::-1]\n",
    "\n",
    "    #             for t in range(no_timesteps): \n",
    "    #                 episodic_advantages[ep_i][i][t] = advantages[t] # append it to the larger episodic tensor\n",
    "    #                 episodic_returns[ep_i][i][t] = returns[t]\n",
    "    #                 episodic_pred_values[ep_i][i][t] = pred_values[t]\n",
    "\n",
    "    #         # append other data to large episode tensor\n",
    "    #         episodic_obs[ep_i] = observations_trajec\n",
    "    #         episodic_actions[ep_i] = actions_trajec\n",
    "    #         episodic_rewards[ep_i] = rewards_trajec\n",
    "    #         episodic_pred_probs[ep_i] = pred_prob_trajec\n",
    "\n",
    "    #     return episodic_obs, episodic_actions, episodic_rewards, returns, episodic_advantages, episodic_pred_probs, episodic_pred_values\n",
    "\n",
    "    def generate_episodes_np(self, observations, no_episodes, no_timesteps) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        '''Generate multiple episodes returning obs, actions, rewards, advantages tensors over all the episodes'''\n",
    "\n",
    "        episodic_advantages = np.zeros((self.num_agents, no_episodes, no_timesteps))\n",
    "        episodic_obs = np.zeros((self.num_agents, no_episodes, no_timesteps, 84))\n",
    "        episodic_rewards = np.zeros((self.num_agents, no_episodes, no_timesteps))\n",
    "        episodic_actions = np.zeros((self.num_agents, no_episodes, no_timesteps))\n",
    "\n",
    "        episodic_returns = np.zeros((self.num_agents, no_episodes, no_timesteps))\n",
    "        episodic_pred_probs = np.zeros((self.num_agents, no_episodes, no_timesteps))\n",
    "        episodic_pred_values = np.zeros((self.num_agents, no_episodes, no_timesteps))\n",
    "\n",
    "        for ep_i in range(no_episodes): \n",
    "            observations_trajec, actions_trajec, rewards_trajec, pred_prob_trajec = self._generate_trajectory_np(observations, no_timesteps)\n",
    "\n",
    "            for agent_i, agent_id in enumerate(self.agent_ids):\n",
    "                \n",
    "                reversed_advantages = np.zeros((no_timesteps))\n",
    "                reversed_returns =  np.zeros((no_timesteps))\n",
    "                reversed_pred_values = np.zeros((no_timesteps))\n",
    "\n",
    "                running_returns = 0\n",
    "\n",
    "                for t in reversed(range(no_timesteps)): # t is reversed here \n",
    "                    \n",
    "                    # calculate returns from rewards \n",
    "                    rewards = rewards_trajec[agent_i][t]\n",
    "                    running_returns += rewards # we have not implemented a 1 step return, currently it consists of entire return. \n",
    "                    reversed_returns[agent_i] = running_returns \n",
    "\n",
    "                    agent_observations_t = observations_trajec[agent_i][t]\n",
    "                    agent_pred_value = self.generate_value(agent_id, agent_observations_t) # simple forward pass in network to calculate value of state. \n",
    "\n",
    "                    advantage = running_returns - agent_pred_value\n",
    "\n",
    "                    reversed_advantages[t] = advantage\n",
    "                    reversed_pred_values[t] = agent_pred_value\n",
    "\n",
    "                advantages = reversed_advantages[::-1]\n",
    "                returns = reversed_returns[::-1] \n",
    "                pred_values = reversed_pred_values[::-1]\n",
    "\n",
    "                for t in range(no_timesteps): \n",
    "                    episodic_advantages[ep_i][agent_i][t] = advantages[t] # append it to the larger episodic tensor\n",
    "                    episodic_returns[ep_i][agent_i][t] = returns[t]\n",
    "                    episodic_pred_values[ep_i][agent_i][t] = pred_values[t]\n",
    "\n",
    "                # append other data to large episode tensor\n",
    "                episodic_obs[agent_i][ep_i] = observations_trajec[agent_i]\n",
    "                episodic_actions[agent_i][ep_i] = actions_trajec[agent_i]\n",
    "                episodic_rewards[agent_i][ep_i] = rewards_trajec[agent_i]\n",
    "                episodic_pred_probs[agent_i][ep_i] = pred_prob_trajec[agent_i]\n",
    "\n",
    "        return episodic_obs, episodic_actions, episodic_rewards, episodic_returns, episodic_advantages, episodic_pred_probs, episodic_pred_values\n",
    "\n",
    "    def __compute_policy_loss(self, old_log_probs, new_log_probs, advantages):\n",
    "        '''Takes sequence of log_probs and advantages, calculates J(0) which is the prob ratios * advantages'''\n",
    "\n",
    "        # Calculate the ratio of new and old probabilities\n",
    "        ratios = torch.exp(new_log_probs - old_log_probs)\n",
    "\n",
    "        # Calculate surrogate loss\n",
    "        surr1 = ratios * advantages\n",
    "        surr2 = torch.clamp(ratios, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()  # Negative because we perform gradient ascent\n",
    "\n",
    "        return policy_loss\n",
    "    \n",
    "    def __compute_value_loss(self, actual_returns, predicted_values) -> torch.Tensor:\n",
    "\n",
    "        # Mean squared error loss between predicted and actual returns\n",
    "        value_loss = F.mse_loss(predicted_values, actual_returns)\n",
    "        \n",
    "        return value_loss\n",
    "\n",
    "    def update_agents_neuralnetwork_old():\n",
    "        pass\n",
    "    \n",
    "    def update_agents_neuralnetwork_old(self):\n",
    "        '''This function does a deep copy of the current agent policies, returning dictionaries'''\n",
    "\n",
    "        self.agents_neuralnetwork_old = [{\"policy\": self.agents_neuralnetwork[i]['policy'].deepCopy(), \"value\": self.agents_neuralnetwork[i]['value'].deepCopy(), \\\n",
    "                                         \"policy_opt\": self.agents_neuralnetwork[i]['policy_opt'].deepCopy(), \\\n",
    "                                            \"value_opt\": self.agents_neuralnetwork[i]['value_opt'].deepCopy()}\\\n",
    "                                         for i in range(self.num_agents)]\n",
    "        \n",
    "        return self.agents_neuralnetwork_old\n",
    "\n",
    "\n",
    "    def update_network(self, agent_enumer, observations_batch, pred_probs_batch, advantages_batch, returns_batch, pred_values_batch):\n",
    "        '''Expects data in tensor format, returns in tensor format'''\n",
    "    \n",
    "        agent_i, agent_id = agent_enumer # unwrap tuple(int,str)\n",
    "\n",
    "        old_pred_probs_batch = np.zeros((len(observations_batch), 4))\n",
    "        \n",
    "        for i, observations in enumerate(observations_batch):\n",
    "            old_pred_probs_batch[i] = self.agents_neuralnetwork_old[agent_i]['policy'](observations) # calc predicted probabilities from the old network \n",
    "        \n",
    "        policy_loss = self.__compute_policy_loss(old_pred_probs_batch, pred_probs_batch, advantages_batch)\n",
    "        \n",
    "        # Backpropagate policy loss\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optimizer.step()\n",
    "\n",
    "        # Calculate value loss\n",
    "        value_loss = self.__compute_value_loss(returns_batch, pred_values_batch)\n",
    "\n",
    "        # Backpropagate value loss\n",
    "        self.value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        self.value_optimizer.step()\n",
    "\n",
    "        return policy_loss, value_loss \n",
    "\n",
    "    def train(self, minibatch_size, no_episodes, no_timesteps, no_epochs):\n",
    "        ''''MA Training happens here. Updates the network, keeping track of metrics throughout'''\n",
    "\n",
    "        train_info_index = 0\n",
    "\n",
    "        batch_size = no_episodes * no_timesteps # e.g. 4*100*100 -> 40000 this is due to flattening the vectors.  \n",
    "\n",
    "        sampler_size = batch_size // minibatch_size # from predicting size \n",
    "\n",
    "        # All training data will go here \n",
    "        train_info = {id:{'policy_loss': np.zeros((no_epochs*sampler_size)), 'value_loss':np.zeros((no_epochs*sampler_size))} for id in self.agent_ids}\n",
    "\n",
    "        for epoch_i in range(no_epochs):\n",
    "            \n",
    "            episodic_obs, episodic_actions, episodic_rewards, episodic_returns, episodic_advantages, \\\n",
    "                episodic_pred_probs, episodic_pred_values = self.generate_episodes_np(multi_agent_env, no_episodes, no_timesteps)\n",
    "            \n",
    "            # episodic_obs (no_agents, no_epi, time_steps, dim)\n",
    "\n",
    "            subset_sampler = SubsetRandomSampler(range(batch_size)) # random assort integers from 1 - 84, put in list [3, 4, 9, 84, ...]\n",
    "            sampler = BatchSampler(subset_sampler, minibatch_size, True) # divide this list into batches of size minibatch_size [1, 3, ..], [54, 76, 2..]\n",
    "\n",
    "            for agent_i, agent_id in enumerate(self.agent_ids): # update network for every agent\n",
    "                agent_obs = episodic_obs[agent_id].reshape(-1,84)\n",
    "\n",
    "                agent_returns = episodic_returns[agent_id].reshape(-1) # (no_epi * time_steps) 1dim\n",
    "                agent_pred_values = episodic_pred_values[agent_id].reshape(-1)\n",
    "\n",
    "                agent_pred_probs = episodic_pred_probs[agent_id].reshape(-1) # (no_epi, time_steps, dim) -> (no_epi*timesteps, dim)\n",
    "                agent_advantages = episodic_advantages[agent_id].reshape(-1)\n",
    "\n",
    "                for k, indices in enumerate(sampler):\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        agent_obs_batch = torch.tensor(agent_obs[indices]) # shape = (len(indices), 84)\n",
    "\n",
    "                        agent_pred_probs_batch = torch.tensor(agent_pred_probs[indices])\n",
    "                        agent_advantages_batch = torch.tensor(agent_advantages[indices])\n",
    "\n",
    "                        agent_returns_batch = torch.tensor(agent_returns[indices])\n",
    "                        agent_pred_values_batch = torch.tensor(agent_pred_values[indices])\n",
    "\n",
    "                    policy_loss, value_loss = self.update_network((agent_i, agent_id), agent_obs_batch, agent_pred_probs_batch, \\\n",
    "                                                                  agent_advantages_batch, agent_returns_batch, agent_pred_values_batch)\n",
    "                    \n",
    "\n",
    "                    train_info[agent_id]['policy_loss'][train_info_index+k] = policy_loss\n",
    "                    train_info[agent_id]['value_loss'][train_info_index+k] = value_loss\n",
    "\n",
    "            train_info_index += len(sampler)\n",
    "        \n",
    "        return train_info   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'aec_to_parallel_wrapper' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[221], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m ppo_algo \u001b[38;5;241m=\u001b[39m PPO(multi_agent_env, \u001b[38;5;241m84\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mppo_algo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[220], line 300\u001b[0m, in \u001b[0;36mPPO.train\u001b[0;34m(self, minibatch_size, no_episodes, no_timesteps, no_epochs)\u001b[0m\n\u001b[1;32m    295\u001b[0m train_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mid\u001b[39m:{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolicy_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mzeros((no_epochs\u001b[38;5;241m*\u001b[39msampler_size)), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue_loss\u001b[39m\u001b[38;5;124m'\u001b[39m:np\u001b[38;5;241m.\u001b[39mzeros((no_epochs\u001b[38;5;241m*\u001b[39msampler_size))} \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_ids}\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(no_epochs):\n\u001b[1;32m    299\u001b[0m     episodic_obs, episodic_actions, episodic_rewards, episodic_returns, episodic_advantages, \\\n\u001b[0;32m--> 300\u001b[0m         episodic_pred_probs, episodic_pred_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_episodes_np\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmulti_agent_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_timesteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;66;03m# episodic_obs (no_agents, no_epi, time_steps, dim)\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     subset_sampler \u001b[38;5;241m=\u001b[39m SubsetRandomSampler(\u001b[38;5;28mrange\u001b[39m(batch_size)) \u001b[38;5;66;03m# random assort integers from 1 - 84, put in list [3, 4, 9, 84, ...]\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[220], line 182\u001b[0m, in \u001b[0;36mPPO.generate_episodes_np\u001b[0;34m(self, observations, no_episodes, no_timesteps)\u001b[0m\n\u001b[1;32m    179\u001b[0m episodic_pred_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_agents, no_episodes, no_timesteps))\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(no_episodes): \n\u001b[0;32m--> 182\u001b[0m     observations_trajec, actions_trajec, rewards_trajec, pred_prob_trajec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_trajectory_np\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_timesteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m agent_i, agent_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_ids):\n\u001b[1;32m    186\u001b[0m         reversed_advantages \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((no_timesteps))\n",
      "Cell \u001b[0;32mIn[220], line 84\u001b[0m, in \u001b[0;36mPPO._generate_trajectory_np\u001b[0;34m(self, observations, timesteps)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_trajectory_np\u001b[39m(\u001b[38;5;28mself\u001b[39m, observations:\u001b[38;5;28mdict\u001b[39m, timesteps:\u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m:\n\u001b[1;32m     81\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Expecting observations for all agents from multi-agent parallel environment setup.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m    Ensure to pass in the current observations of all agents in env'''\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m     observation_trajectories \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_agents, timesteps, \u001b[38;5;28mlen\u001b[39m(\u001b[43mobservations\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)))\n\u001b[1;32m     85\u001b[0m     action_trajectories \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_agents, timesteps))\n\u001b[1;32m     86\u001b[0m     reward_trajectories \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_agents, timesteps))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'aec_to_parallel_wrapper' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "ppo_algo = PPO(multi_agent_env, 84, 4)\n",
    "ppo_algo.train(5, 12, 15, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([15, 31, 53, 23]),\n",
       " array([51, 59, 49, 26]),\n",
       " array([57, 18, 27, 45]),\n",
       " array([ 3, 46, 25,  5])]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(np.zeros((2,2,2,2)))\n",
    "rand = torch.randperm(64).numpy()\n",
    "sampler = [rand[i * 4:(i + 1) * 4] for i in range(4)]\n",
    "\n",
    "sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 26, 3]\n",
      "[7, 10, 31]\n",
      "[2, 0, 12]\n",
      "[19, 8, 13]\n",
      "[1, 28, 23]\n",
      "[27, 22, 24]\n",
      "[21, 15, 5]\n",
      "[6, 17, 25]\n",
      "[9, 30, 20]\n",
      "[18, 14, 11]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "subset_sampler = SubsetRandomSampler(range(32))\n",
    "sampler = BatchSampler(subset_sampler, 3, True)\n",
    "\n",
    "for i, idx in enumerate(sampler):\n",
    "    print(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minibatch_size = 3\n",
    "batch_size = 244444\n",
    "\n",
    "subset_sampler = SubsetRandomSampler(range(batch_size))\n",
    "sampler = BatchSampler(subset_sampler, minibatch_size, True)\n",
    "\n",
    "sampler_size = batch_size // minibatch_size\n",
    "sampler_size == len(sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 9, 2, 3, 4, 8, 1, 6, 7, 5])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.randperm(len(range(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(subset_sampler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3, 1, 18)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.randint(1,43, size =(4,3,1,18))\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.reshape(-1, *a.shape[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor((5)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 18)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rand:  [ 0 10  2  9  6  8  3  5  7  1  4 11]\n",
      "sampler:  [array([ 0, 10,  2,  9,  6]), array([8, 3, 5, 7, 1]), array([ 4, 11]), array([], dtype=int64)]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 12\n",
    "minibatch_size = 5\n",
    "num_mini_batch = 4\n",
    "\n",
    "rand = torch.randperm(batch_size).numpy()\n",
    "sampler = [rand[i * minibatch_size:(i + 1) * minibatch_size] for i in range(num_mini_batch)] # rand[0:4],  rand[4:10], etc\n",
    "print('rand: ', rand)\n",
    "print('sampler: ', sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we are going to use these indices to \n",
    "arrays = []\n",
    "for indices in sampler:\n",
    "    minibatch = b[indices]\n",
    "    arrays.append(minibatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'arrays' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[203], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43marrays\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'arrays' is not defined"
     ]
    }
   ],
   "source": [
    "arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.tensor((2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.max().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "num_agents = 2\n",
    "no_episodes = 3\n",
    "no_timesteps = 5\n",
    "\n",
    "episodic_obs = np.zeros((num_agents, no_episodes, no_timesteps, 2))\n",
    "\n",
    "reward_trajectories = np.ones((num_agents, no_timesteps, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]]]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodic_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]],\n",
       "\n",
       "       [[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]]]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for agent in range(num_agents): \n",
    "    episodic_obs[agent][1] = reward_trajectories[agent]\n",
    "\n",
    "episodic_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]]]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for agent in range(num_agents): \n",
    "    episodic_obs[agent][1] = reward_trajectories[agent][:]\n",
    "\n",
    "episodic_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 4, 4, 4, 3])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([3,3,4,5])\n",
    "a[[1,2,2,2,2,1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.ones((4,5,5,84))\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.reshape(-1,84)\n",
    "len(b)\n",
    "len(b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 84, 1)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_epochs = 5\n",
    "sampler_size = 10\n",
    "\n",
    "\n",
    "train_info = {id:{'policy_loss': np.zeros((no_epochs*sampler_size)), 'value_loss':np.zeros((no_epochs*sampler_size))} for id in multi_agent_env.agents}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_info['1']['policy_loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 30 into shape (84)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[168], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m agent_obs \u001b[38;5;241m=\u001b[39m \u001b[43mepisodic_obs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m84\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 30 into shape (84)"
     ]
    }
   ],
   "source": [
    "agent_obs = episodic_obs[1].reshape(-1,84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodic_obs = np.zeros((3,4,84))\n",
    "episodic_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 84)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(episodic_obs.reshape(-1, 84))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodic_obs.reshape(-1, 84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'aec_to_parallel_wrapper' object has no attribute 'pos'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[190], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmulti_agent_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'aec_to_parallel_wrapper' object has no attribute 'pos'"
     ]
    }
   ],
   "source": [
    "multi_agent_env.pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "observations_batch = np.zeros((5,4))\n",
    "\n",
    "old_log_probs_batch = np.zeros((len(observations_batch), 4))\n",
    "for i, observations in enumerate(observations_batch):\n",
    "    old_log_probs_batch[i] = observations\n",
    "    print(i)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_log_probs_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(torch.tensor(np.zeros((3,4))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
