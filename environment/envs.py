from collections import Counter
import csv
import os
import time
from typing import List, Optional, Union

import numpy as np
from torch.utils.tensorboard import SummaryWriter

from gymnasium.spaces import Discrete
from gymnasium.utils import EzPickle
from pettingzoo.utils import agent_selector

from sumo_rl import SumoEnvironment
from sumo_rl.environment.env import SumoEnvironmentPZ, parallel_env
import traci

import sys 
from datetime import datetime

sys.path.append('/Users/loveen/Desktop/Masters project/rl-multi-agent-traffic-nonexhaust-emissions')
from environment.helper_functions import get_total_waiting_time, get_tyre_pm


LIBSUMO = "LIBSUMO_AS_TRACI" in os.environ

# This file was taken from repo, https://github.com/case547/masters-proj/blob/master/envs.py, and has been altered to suit needs 

# this class ensures that we count rewards, instead of bypassing that delta_time period, it counts the rewards during delta time, 
# and returns this reward_hold dict as the computed rewards 
# overrides step(), _run_steps() and _compute_rewards()
class SumoEnvironmentCountAllRewards(SumoEnvironment):
    """Environment that counts rewards every sumo_step.
    
    Because delta_time != 1, the reward given to the agent(s) every
    step() is the sum of the last delta_time rewards generated by SUMO.
    """
    def __init__(self, eval_mode=False, csv_path:Optional[str]=None, tb_log_dir:Optional[str]=None, **kwargs):
        # Call the parent constructor
        super().__init__(**kwargs)
        
        self.eval_mode = eval_mode

        # if the eval_mode is batch, then i want to plot mean values on tensorboard + store mean values on csv
        # if the env mode is single, then i want to plot all values on tensorboard

        # if the eval_mode is single, then i want to plot all values, but also store mean values on csv
        self.single_eval_mode = 
        self.batch_eval_mode = 

        if self.single_eval_mode or self.batch_eval_mode : 
            assert csv_path is not None, f"csv path not given, needed in order to save metrics"
            self.csv_path = csv_path
        
        # else: # if its just training phase 
        #     current_time = datetime.now().strftime("%Y-%m-%d_%H_%M")
        #     self.csv_path = os.path.join(csv_path, f"training_metrics_{current_time}.csv")

        self.tb_log_dir = tb_log_dir

        if tb_log_dir:
            self.tb_writer = SummaryWriter(tb_log_dir)  # prep TensorBoard

        # if not self.eval_mode:
        #     assert train_sim_period is not None, f"train_sim_period is {train_sim_period} \
        #     must be an integer"
        #     self.train_sim_period = train_sim_period
            
        # Initialise cumulative system counters
        self.episode_sys_abs_accel = 0
        self.episode_sys_arrived_so_far = 0

        self.ts_summary_world_metrics_dims = 2 # change according to what is above 

        # Initialise episodic summary counters
        self.episode_abs_accel = [0] * len(self.ts_ids)
        self.episode_avg_speed = [0] * len(self.ts_ids)
        self.episode_accumulated_waiting_time = [0] * len(self.ts_ids)
        self.episode_stopped = [0] * len(self.ts_ids)
        self.episode_reward_total = [0] * len(self.ts_ids)

        self.ts_summary_agent_metrics_dims = 5 * len(self.ts_ids)

        # lanes visible to all ts
        self.controlled_lanes = [] # lo this will end up being an array of num_lanes controlled for each traffic signal. wierd how its a list, should be dict to preserve key value
        for ts in self.traffic_signals.values():
            self.controlled_lanes += ts.lanes

        self.max_dist = 200

        if csv_path:
            with open(self.csv_path, "w", newline="") as f:
                csv_writer = csv.writer(f, lineterminator='\n')
                headers = ["sim_time", "episode_num"]
                
                system_metrics = [ "system_abs_accel", 
                                  "sys_total_stopped",
                                  "sys_total_wait", 
                                  "sys_avg_wait", 
                                  "sys_avg_speed", 
                                  "sys_arrived_vehicles_at_destination"] 
                
                headers += system_metrics 

                for ts in self.ts_ids:
                    ts_metrics = [f"{ts}_abs_accel", 
                                  f"{ts}_stopped", 
                                  f"{ts}_average_speed",
                                  f"{ts}_accumulated_waiting_time"]
                    headers += ts_metrics
                
                for ts in self.ts_ids:
                    ts_summary_metrics = [f"{ts}_episode_abs_accel",
                                  f"{ts}_episode_average_speed",
                                  f"{ts}_episode_accumulated_waiting_time",
                                  f"{ts}_episode_stopped"
                                  f"{ts}_episode_reward_total"]
                    
                    headers += ts_summary_metrics

                headers = (headers)
                csv_writer.writerow(headers)

    def _get_system_info(self):
        vehicles = self.sumo.vehicle.getIDList()
        speeds = [self.sumo.vehicle.getSpeed(vehicle) for vehicle in vehicles]
        waiting_times = [self.sumo.vehicle.getWaitingTime(vehicle) for vehicle in vehicles]
        
        system_tyre_pm = get_tyre_pm()  # abs accelerations of all vehicles in network 
        system_arrived_num = self.sumo.simulation.getArrivedNumber() 
        
        system_stats = {
            "system_abs_accel": system_tyre_pm,
            "system_total_stopped": sum(int(speed < 0.1) for speed in speeds),
            "system_total_waiting_time": sum(waiting_times),
            "system_avg_waiting_time": 0.0 if len(vehicles) == 0 else np.mean(waiting_times),
            "system_avg_speed": 0.0 if len(vehicles) == 0 else np.mean(speeds),
            "system_arrived_vehicles_at_destination": system_arrived_num
        }

        return system_stats  
    
        # return {
        #     # In SUMO, a vehicle is considered halting if its speed is below 0.1 m/s
        #     "system_total_stopped": sum(int(speed < 0.1) for speed in speeds),
        #     "system_total_waiting_time": sum(waiting_times),
        #     "system_mean_waiting_time": 0.0 if len(vehicles) == 0 else np.mean(waiting_times),
        #     "system_mean_speed": 0.0 if len(vehicles) == 0 else np.mean(speeds),
        #     "system_tyre_pm": get_tyre_pm()
        # }
    
    def _get_per_agent_info(self):
        stopped = [self.traffic_signals[ts].get_total_queued() for ts in self.ts_ids]
        accumulated_waiting_time = [
            sum(self.traffic_signals[ts].get_accumulated_waiting_time_per_lane()) for ts in self.ts_ids
        ]
        average_speed = [self.traffic_signals[ts].get_average_speed() for ts in self.ts_ids]
        abs_accelerations = [get_tyre_pm(ts) for ts in self.traffic_signals.values()]
        info = {}
        for i, ts in enumerate(self.ts_ids):
            info[f"{ts}_stopped"] = stopped[i]
            info[f"{ts}_accumulated_waiting_time"] = accumulated_waiting_time[i]
            info[f"{ts}_average_speed"] = average_speed[i]
            info[f"{ts}_abs_accel"] = abs_accelerations[i]
        info["agents_total_stopped"] = sum(stopped)
        info["agents_total_accumulated_waiting_time"] = sum(accumulated_waiting_time)
        return info
    
    def _sumo_step(self):
        self.sumo.simulationStep() # lo. this is solely offered in the original function 
        
        # collect system + agent metrics for every sumo simulation second
        system_stats = self.compute_system_metrics() 
        agent_stats = self.compute_agent_metrics()
        
        if self.eval_mode:
            # TODO: specify that this is only episodic evaluation mode
            if self.sim_step == self.sim_max_time: # assuming that max_time is max_time in one episode 
                summary_episode_agent_stats = self.compute_episode_agent_stats(env_episodic=True)
                summary_episode_world_stats = self.compute_episode_world_stats(env_episodic=True)

                if hasattr(self, "tb_writer"):
                    self.log_tensor_summary_metrics(summary_episode_agent_stats, summary_episode_world_stats, env_episodic=True, label="eval")
            
                self.reset_episodic_stats()

            else:
                summary_episode_agent_stats = None 
                summary_episode_world_stats = None
            
            # store values over multiple episodes in in csv file 
            self.log_csv_step_metrics(system_stats, 
                                      agent_stats,
                                      summary_episode_agent_stats,
                                      summary_episode_world_stats)
        
        # else:
        #     # TODO: emphasise somewhere that this is the inf_horizon_training_summary_mode -> 
        #     # plot every training simulation period - suited for infinite horizon environment
        #     if self.sim_step % self.train_sim_period == 0: 
        #         summary_episode_agent_stats = self.compute_episode_agent_stats(env_episodic=False)
        #         summary_episode_world_stats = self.compute_episode_world_stats(env_episodic=False)

        #         if hasattr(self, "tb_writer"):
        #             self.log_tensor_summary_metrics(summary_episode_agent_stats, summary_episode_world_stats, env_episodic=False, label="train")
                
        #         self.reset_episodic_stats() # reset counters after training ends

        #     else:
        #         summary_episode_agent_stats = None 
        #         summary_episode_world_stats = None
            
        #     # store values over multiple episodes in in csv file 
        #     self.log_csv_step_metrics(system_stats, 
        #                               agent_stats,
        #                               summary_episode_agent_stats,
        #                               summary_episode_world_stats) 

        # if self.single_infinite_horizon_eval_mode:
        # # in this mode, in additional to above -> we are interested in recording the step by step 
        # # metrics. 
        #     self.log_tensor_step_metrics()

    def compute_system_metrics(self):
        '''collect system metrics. Should be called for every SUMO simulation second'''
        
        # SYSTEM - all vehicles in network 
        vehicles = self.sumo.vehicle.getIDList() # get all vehicles in env
        speeds = [self.sumo.vehicle.getSpeed(veh) for veh in vehicles] # [speedv1 speedv2]
        waiting_times = [self.sumo.vehicle.getWaitingTime(veh) for veh in vehicles] # [waitv1 waitv2]

        system_tyre_pm = get_tyre_pm()  # abs accelerations of all vehicles in network 
        system_arrived_num = self.sumo.simulation.getArrivedNumber() 
        
        # append to cumulative counters - absolute accelerations
        self.episode_sys_abs_accel += system_tyre_pm 
        self.episode_sys_arrived_so_far += system_arrived_num 
        
        system_stats = {
            "system_abs_accel": system_tyre_pm,
            "system_total_stopped": sum(int(speed < 0.1) for speed in speeds),
            "system_total_waiting_time": sum(waiting_times),
            "system_avg_waiting_time": 0.0 if len(vehicles) == 0 else np.mean(waiting_times),
            "system_avg_speed": 0.0 if len(vehicles) == 0 else np.mean(speeds),
            "system_arrived_vehicles_at_destination": system_arrived_num
        }
        
        return system_stats
    
    def compute_agent_metrics(self):
        '''log agent related metrics - to be used within every SUMO simulation step''' 
        
        stopped = [self.traffic_signals[ts].get_total_queued() for ts in self.ts_ids]
        accumulated_waiting_time = [
            sum(self.traffic_signals[ts].get_accumulated_waiting_time_per_lane()) for ts in self.ts_ids
        ]
        average_speed = [self.traffic_signals[ts].get_average_speed() for ts in self.ts_ids]
        abs_accelerations = [get_tyre_pm(ts) for ts in self.traffic_signals.values()]
        
        # append to agent-specific cumulative counters 
        for i, ts in enumerate(self.ts_ids):
            self.episode_abs_accel[i] += abs_accelerations[i]
            self.episode_stopped[i] += stopped[i]
            self.episode_avg_speed[i] += average_speed[i]
            self.episode_accumulated_waiting_time[i] += accumulated_waiting_time[i]

            # keep record of total reward 
            if self.rewards[ts] is not None:
                self.episode_reward_total[i] += self.rewards[ts] 

        # store values in dictionaries 
        agent_stats = {}
        for i, ts in enumerate(self.ts_ids):
            agent_stats[f"{ts}_abs_accel"] = abs_accelerations[i]
            agent_stats[f"{ts}_stopped"] = stopped[i]
            agent_stats[f"{ts}_average_speed"] = average_speed[i]
            agent_stats[f"{ts}_accumulated_waiting_time"] = accumulated_waiting_time[i]

        return agent_stats

    def compute_episode_agent_stats(self, env_episodic=True):
        '''Compute total/average of summary class variables
            Should be called only at the end of an episode or period of infinite horizon.
            if env_episodic=True, it will plot per episode, and if the env is not episodic,
            it will plot for every sim second'''

        # either returns sumo time period over an episode or
        # returns sumo time period equal to training period 
        if env_episodic: 
            assert self.sim_step == self.sim_max_time, \
                f"Simulation step {self.sim_step} does not equal the maximum simulation time {self.sim_max_time}"
            time_period = self.sim_max_time
        else: 
            assert hasattr(self, self.train_sim_period), f"Object does not have the attribute {str(self.train_sim_period)}"
            assert self.sim_step % self.train_sim_period == 0, f"self.sim_step: {self.sim_step} is not a multiple of train_sim_period: {self.train_sim_period}"

            time_period = self.train_sim_period
                    
        summary_episode_agent_stats = {}
        
        for i, ts in enumerate(self.ts_ids):
            summary_episode_agent_stats[f"{ts}_episode_abs_accel"] = self.episode_abs_accel[i]
            summary_episode_agent_stats[f"{ts}_episode_avg_speed"] = self.episode_avg_speed[i] / time_period
            summary_episode_agent_stats[f"{ts}_episode_accumulated_waiting_time"] = self.episode_accumulated_waiting_time[i] / time_period
            summary_episode_agent_stats[f"{ts}_episode_stopped"] = self.episode_stopped[i] / time_period
            summary_episode_agent_stats[f"{ts}_episode_reward_total"] = self.episode_reward_total[i]
        
        return summary_episode_agent_stats
   
    def compute_episode_world_stats(self, env_episodic=True):

        # either returns sumo time period over an episode or
        # returns sumo time period equal to training period 
        if env_episodic: 
            assert self.sim_step == self.sim_max_time, \
                f"Simulation step {self.sim_step} does not equal the maximum simulation time {self.sim_max_time}"
            time_period = self.sim_max_time
        else: 
            assert hasattr(self, self.train_sim_period), f"Object does not have the attribute {str(self.train_sim_period)}"
            assert self.sim_step % self.train_sim_period == 0, f"self.sim_step: {self.sim_step} is not a multiple of train_sim_period: {self.train_sim_period}"

            time_period = self.train_sim_period

        summary_episode_world_stats = {}
        summary_episode_world_stats[f"episode_sys_abs_accel"] = self.episode_sys_abs_accel / time_period
        summary_episode_world_stats[f"episode_sys_arrived_so_far"] = self.episode_sys_arrived_so_far / time_period
        
        return summary_episode_world_stats
    
    def reset_episodic_stats(self):
        '''reset the episodic variables'''
        # Initialise cumulative system counters
        self.episode_sys_abs_accel = 0
        self.episode_sys_arrived_so_far = 0

        # Initialise episodic summary counters
        self.episode_abs_accel = [0] * len(self.ts_ids)
        self.episode_avg_speed = [0] * len(self.ts_ids)
        self.episode_accumulated_waiting_time = [0] * len(self.ts_ids)
        self.episode_stopped = [0] * len(self.ts_ids)
        self.episode_reward_total = [0] * len(self.ts_ids)

    def log_csv_step_metrics(self,  system_stats, agent_stats, summary_episode_world_stats, summary_episode_agent_stats):
        '''log metrics to csv file. If data is none, then plot all zeros'''
        # assert self.sim_step == self.sim_max_time, f'currrent step {self.sim_step} is not equal to {self.sim_max_time}'

        if self.csv_path:
            with open(self.csv_path, "a", newline="", ) as f:
                csv_writer = csv.writer(f, lineterminator='\n')
                
                if not summary_episode_world_stats and not summary_episode_agent_stats: # if these are both Nones 
                    data = ([self.sim_step, self.episode]
                            + list(system_stats.values())
                            + list(agent_stats.values())
                            + [0 for i in range(self.ts_summary_world_metrics_dims)]
                            + [0 for i in range(self.ts_summary_agent_metrics_dims)])
                else: 
                    data = ([self.sim_step, self.episode]
                            + list(system_stats.values())
                            + list(agent_stats.values())
                            + list(summary_episode_world_stats.values())
                            + list(summary_episode_agent_stats.values()))
                    
                csv_writer.writerow(data)

    def log_tensor_summary_metrics(self, summary_episode_agent_stats, summary_episode_world_stats, env_episodic, label): 
        '''log the summary metrics at end of every episode. Should be used during training'''
        if env_episodic:  # TODO: make this a part of class variable 
            x = self.episode
        else:
            x = self.sim_step

        # Log to TensorBoard
        assert (self.sim_step == self.sim_max_time or self.sim_step % self.train_sim_period == 0), \
            f"Simulation step {self.sim_step} does not equal the maximum simulation time, {self.sim_max_time} \
            or is not a multiple of the train simulation period, {self.train_sim_period}"
        
        for stat, val in summary_episode_world_stats.items():
                self.tb_writer.add_scalar(f"{label}/system/{stat}", val, x)
    
        for stat, val in summary_episode_agent_stats.items():
                self.tb_writer.add_scalar(f"{label}/agent_{stat[0]}/{stat[2:]}", val, x)
            
    def log_tensor_step_metrics(self, agent_stats, system_stats, label):
        '''record metrics for every step in the system. Should be used when running single evaluations.'''
        if hasattr(self, "tb_writer"):
            for stat, val in system_stats.items():
                self.tb_writer.add_scalar(f"{label}/system/{stat}", val, self.sim_step)

            for stat, val in agent_stats.items():
                self.tb_writer.add_scalar(f"{label}/agent_{stat[0]}/{stat[2:]}", val, self.sim_step)

        # self.tb_writer.add_scalar("world/abs_accel_cumulative", self.episode_sys_abs_accel, self.sim_step)
        # self.tb_writer.add_scalar("world/episode_sys_arrived_so_far", self.episode_sys_arrived_so_far, self.sim_step)    
        
    def step(self, action: Union[dict, int]):
        """Apply the action(s) and then step the simulation for delta_time seconds.

        Args:
            action (Union[dict, int]): action(s) to be applied to the environment.
            If single_agent is True, action is an int, otherwise it expects a dict with keys corresponding to traffic signal ids.
        """
        # No action, follow fixed TL defined in self.phases
        if action is None or action == {}:
            # Rewards for the sumo steps between every env step
            self.reward_hold = Counter({ts: 0 for ts in self.ts_ids})
            for _ in range(self.delta_time):
                self._sumo_step()

                r = {ts: self.traffic_signals[ts].compute_reward() for ts in self.ts_ids}
                self.reward_hold.update(r)  # add r to reward_hold Counter
        else:
            self._apply_actions(action)
            self._run_steps()

        observations = self._compute_observations()
        rewards = self._compute_rewards()
        dones = self._compute_dones()
        terminated = False  # there are no 'terminal' states in this environment
        truncated = dones["__all__"]  # episode ends when sim_step >= max_steps
        info = self._compute_info()

        if self.single_agent:
            return observations[self.ts_ids[0]], rewards[self.ts_ids[0]], terminated, truncated, info
        else:
            return observations, rewards, dones, info
    
    def _run_steps(self): # this func is also called in step() function 
        # Rewards for the sumo steps between every env step
        self.reward_hold = Counter({ts: 0 for ts in self.ts_ids})
        time_to_act = False
        while not time_to_act:
            self._sumo_step()
            r = {ts: self.traffic_signals[ts].compute_reward() for ts in self.ts_ids}
            self.reward_hold.update(r)  # add r to reward_hold Counter

            for ts in self.ts_ids:
                self.traffic_signals[ts].update()
                if self.traffic_signals[ts].time_to_act:
                    time_to_act = True

    def _compute_rewards(self): # this func is called in step()
        self.rewards.update(
            {ts: self.reward_hold[ts] for ts in self.ts_ids if self.traffic_signals[ts].time_to_act}
        )
        return {ts: self.rewards[ts] for ts in self.rewards.keys() if self.traffic_signals[ts].time_to_act} # same thing as returning rewards_hold ... why not return rewards_hold?

    def _compute_info(self):
        info = {"__common__": {"step": self.sim_step}}
        per_agent_info = self._get_per_agent_info()

        for agent_id in self.ts_ids:
            agent_info = {}

            for k, v in per_agent_info.items():
                if k.startswith(agent_id):
                    agent_info[k.split("_")[-1]] = v

            # Add tyre PM
            agent_info["tyre_pm"] = get_tyre_pm(self.traffic_signals[agent_id])

            info.update({agent_id: agent_info})

        return info

    def get_observable_vehs(self, lane) -> List[str]:
        """Remove undetectable vehicles from a lane."""
        detectable = []
        for vehicle in self.sumo.lane.getLastStepVehicleIDs(lane): # Returns the ids of the vehicles for the last time step on the given lane
            path = self.sumo.vehicle.getNextTLS(vehicle) # Return list of upcoming traffic lights [(tlsID, tlsIndex, distance, state), ...]
            if len(path) > 0:
                next_light = path[0]
                distance = next_light[2]
                if distance <= self.max_dist:  # Detectors have a max range. if distance from vehicle to traffic light is < 200, vehicle is detectable from traffic light
                    detectable.append(vehicle)
        return detectable
    
    def reset(self, seed: Optional[int] = None, **kwargs):
        """Reset the environment."""
        super().reset(seed=seed, **kwargs)

        # reset cumulative counters
        self.episode_sys_abs_accel = 0
        self.episode_sys_arrived_so_far = 0
        
        # reset episodic summary counters
        self.episode_abs_accel = [0] * len(self.ts_ids)
        self.episode_avg_speed = [0] * len(self.ts_ids)
        self.episode_accumulated_waiting_time = [0] * len(self.ts_ids)
        self.episode_stopped = [0] * len(self.ts_ids)
        self.episode_reward_total = [0] * len(self.ts_ids)

# class same as SumoEnvironmentPZ, (as it inherits from sumoEnvPZ), BUT THIS TIME, 
# instead of self.env = SumoEnvironment(), its self.env = SumoEnvironmentCountAllRewards()
class SumoEnvironmentPZCountAllRewards(SumoEnvironmentPZ):
    """A wrapper for `SumoEnvironmentCountAllRewards` subclasses SumoEnvironmentPZ"""
    def __init__(self, eval_mode=False, csv_path: Optional[str] = None, tb_log_dir: Optional[str] = None, **kwargs):
        EzPickle.__init__(self, **kwargs)
        self._kwargs = kwargs

        self.seed()
        self.env = SumoEnvironmentCountAllRewards(**self._kwargs, eval_mode=eval_mode, csv_path=csv_path, tb_log_dir=tb_log_dir)  # instead of SumoEnvironment. CountAllRewardsEnv is subclass of SumoEnv  

        self.agents = self.env.ts_ids  # do you really need to redefine all the attributes, they are already inherited from SumoEnvPz?
        self.possible_agents = self.env.ts_ids
        self._agent_selector = agent_selector(self.agents)
        self.agent_selection = self._agent_selector.reset()
        # spaces
        self.action_spaces = {a: self.env.action_spaces(a) for a in self.agents}
        self.observation_spaces = {a: self.env.observation_spaces(a) for a in self.agents}

        # dicts
        self.rewards = {a: 0 for a in self.agents}
        self.terminations = {a: False for a in self.agents}
        self.truncations = {a: False for a in self.agents}
        self.infos = {a: {} for a in self.agents}
        

    # @property
    # def action_space(self):
    #     """Return the biggest action space of the traffic signal agents."""
    #     spaces = [self.action_spaces[ts] for ts in self.env.traffic_signals]
    #     max_n = max([space.n for space in spaces])
    #     return Discrete(max_n)

    # def close(self):
    #     """Close the environment and stop the SUMO simulation."""
    #     if self.env.sumo is None:
    #         return

    #     if not LIBSUMO:
    #         traci.switch(self.env.label)
    #     traci.close()

    #     # Help completely release SUMO port between episodes to address
    #     # "Unable to create listening socket: Address already in use" error
    #     time.sleep(2)

    #     if self.env.disp is not None:
    #         self.env.disp.stop()
    #         self.env.disp = None

    #     self.env.sumo = None