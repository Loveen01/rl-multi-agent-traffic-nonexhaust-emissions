from collections import Counter
import csv
import os
import time
from typing import List, Optional, Union

import numpy as np
from torch.utils.tensorboard import SummaryWriter

from gymnasium.spaces import Discrete
from gymnasium.utils import EzPickle
from pettingzoo.utils import agent_selector

from sumo_rl import SumoEnvironment
from sumo_rl.environment.env import SumoEnvironmentPZ, parallel_env
import traci

import sys 
from datetime import datetime

sys.path.append('/Users/loveen/Desktop/Masters project/rl-multi-agent-traffic-nonexhaust-emissions')
from environment.helper_functions import get_total_waiting_time, get_tyre_pm


LIBSUMO = "LIBSUMO_AS_TRACI" in os.environ

# This file was taken from repo, https://github.com/case547/masters-proj/blob/master/envs.py, and has been altered to suit needs 

# this class ensures that we count rewards, instead of bypassing that delta_time period, it counts the rewards during delta time, 
# and returns this reward_hold dict as the computed rewards 
# overrides step(), _run_steps() and _compute_rewards()
class SumoEnvironmentCountAllRewards(SumoEnvironment):
    """Environment that counts rewards every sumo_step.
    
    Because delta_time != 1, the reward given to the agent(s) every
    step() is the sum of the last delta_time rewards generated by SUMO.
    """
    def __init__(self, single_eval_mode: bool = False, multiple_eval_mode : bool = False, csv_path:Optional[str]=None, tb_log_dir:Optional[str]=None, **kwargs):
        # Call the parent constructor
        super().__init__(**kwargs)
        
        self.single_eval_mode = single_eval_mode

        # both of these cannot be set to true.
        assert not (single_eval_mode and multiple_eval_mode), "single_eval_mode and multiple_eval_mode are both true"
                  
        self.single_eval_mode = single_eval_mode
        self.multiple_eval_mode = multiple_eval_mode

        if self.single_eval_mode or self.multiple_eval_mode : 
            assert csv_path is not None, f"csv path not given, needed in order to save metrics"
            self.csv_path = csv_path
        
        self.tb_log_dir = tb_log_dir

        if tb_log_dir:
            self.tb_writer = SummaryWriter(tb_log_dir)  # prep TensorBoard

        # Initialise cumulative system counters
        self.episode_sys_abs_accel = 0
        self.episode_sys_arrived_so_far = 0

        self.ts_summary_world_metrics_dims = 2 # change according to what is above 

        # Initialise episodic summary counters
        self.episode_abs_accel = [0] * len(self.ts_ids)
        self.episode_avg_speed = [0] * len(self.ts_ids)
        self.episode_accumulated_waiting_time = [0] * len(self.ts_ids)
        self.episode_stopped = [0] * len(self.ts_ids)
        self.episode_reward_total = [0] * len(self.ts_ids)

        self.ts_summary_agent_metrics_dims = 5 * len(self.ts_ids)

        # lanes visible to all ts
        self.controlled_lanes = [] # lo this will end up being an array of num_lanes controlled for each traffic signal. wierd how its a list, should be dict to preserve key value
        for ts in self.traffic_signals.values():
            self.controlled_lanes += ts.lanes

        self.max_dist = 200

        if csv_path:
            with open(self.csv_path, "w", newline="") as f:
                csv_writer = csv.writer(f, lineterminator='\n')
                headers = ["sim_time", "episode_num"]
                
                system_metrics = [ "system_abs_accel", 
                                  "sys_total_stopped",
                                  "sys_total_wait", 
                                  "sys_avg_wait", 
                                  "sys_avg_speed", 
                                  "sys_arrived_vehicles_at_destination",
                                  "sys_accum_waiting_time"] 
                
                headers += system_metrics 

                for ts in self.ts_ids:
                    ts_metrics = [f"{ts}_abs_accel", 
                                  f"{ts}_stopped", 
                                  f"{ts}_average_speed",
                                  f"{ts}_accumulated_waiting_time"]
                    headers += ts_metrics
                
                for ts in self.ts_ids:
                    ts_summary_metrics = [f"{ts}_episode_abs_accel",
                                  f"{ts}_episode_average_speed",
                                  f"{ts}_episode_accumulated_waiting_time",
                                  f"{ts}_episode_stopped",
                                  f"{ts}_episode_reward_total"]
                    
                    headers += ts_summary_metrics

                headers = (headers)
                csv_writer.writerow(headers)

        # store rewards here -> 
        self.reward_hold = Counter({ts: 0 for ts in self.ts_ids})
        
    def _get_system_info(self):
        vehicles = self.sumo.vehicle.getIDList()
        speeds = [self.sumo.vehicle.getSpeed(vehicle) for vehicle in vehicles]
        waiting_times = [self.sumo.vehicle.getWaitingTime(vehicle) for vehicle in vehicles]
        
        system_tyre_pm = get_tyre_pm()  # abs accelerations of all vehicles in network 
        system_arrived_num = self.sumo.simulation.getArrivedNumber() 
        
        system_stats = {
            "system_abs_accel": system_tyre_pm,
            "system_total_stopped": sum(int(speed < 0.1) for speed in speeds),
            "system_total_waiting_time": sum(waiting_times),
            "system_avg_waiting_time": 0.0 if len(vehicles) == 0 else np.mean(waiting_times),
            "system_avg_speed": 0.0 if len(vehicles) == 0 else np.mean(speeds),
            "system_arrived_vehicles_at_destination": system_arrived_num
        }

        return system_stats  
    
        # return {
        #     # In SUMO, a vehicle is considered halting if its speed is below 0.1 m/s
        #     "system_total_stopped": sum(int(speed < 0.1) for speed in speeds),
        #     "system_total_waiting_time": sum(waiting_times),
        #     "system_mean_waiting_time": 0.0 if len(vehicles) == 0 else np.mean(waiting_times),
        #     "system_mean_speed": 0.0 if len(vehicles) == 0 else np.mean(speeds),
        #     "system_tyre_pm": get_tyre_pm()
        # }
    
    def _get_per_agent_info(self):
        stopped = [self.traffic_signals[ts].get_total_queued() for ts in self.ts_ids]
        accumulated_waiting_time = [
            sum(self.traffic_signals[ts].get_accumulated_waiting_time_per_lane()) for ts in self.ts_ids
        ]
        average_speed = [self.traffic_signals[ts].get_average_speed() for ts in self.ts_ids]
        abs_accelerations = [get_tyre_pm(ts) for ts in self.traffic_signals.values()]
        info = {}
        for i, ts in enumerate(self.ts_ids):
            info[f"{ts}_stopped"] = stopped[i]
            info[f"{ts}_accumulated_waiting_time"] = accumulated_waiting_time[i]
            info[f"{ts}_average_speed"] = average_speed[i]
            info[f"{ts}_abs_accel"] = abs_accelerations[i]
        info["agents_total_stopped"] = sum(stopped)
        info["agents_total_accumulated_waiting_time"] = sum(accumulated_waiting_time)
        return info
    
    def _sumo_step(self):
        self.sumo.simulationStep() # lo. this is solely offered in the original function 
        
        # collect system + agent metrics for every sumo simulation second
        system_stats = self.compute_system_metrics() 
        agent_stats = self.compute_agent_metrics()
        
        if self.multiple_eval_mode or self.single_eval_mode:
            
            if self.sim_step != self.sim_max_time:
                summary_episode_agent_stats = None 

            else: 
                summary_episode_agent_stats = self.compute_episode_agent_stats()
                
                if self.multiple_eval_mode:
                    # if hasattr(self, "tb_writer"):
                    self.log_tensor_summary_metrics(summary_episode_agent_stats,
                                                    label="episodic_eval")

                self.reset_episodic_stats()

            # store values over multiple episodes in in csv file 
            self.log_csv_step_metrics(system_stats, 
                                      agent_stats,
                                      summary_episode_agent_stats)

            if self.single_eval_mode:
                self.log_tensor_step_metrics(agent_stats=agent_stats,
                                             system_stats=system_stats,
                                             label='single_eval')

    def compute_system_metrics(self):
        '''collect system metrics. Should be called for every SUMO simulation second'''
        
        # SYSTEM - all vehicles in network 
        vehicles = self.sumo.vehicle.getIDList() # get all vehicles in env
        speeds = [self.sumo.vehicle.getSpeed(veh) for veh in vehicles] # [speedv1 speedv2]
        waiting_times = [self.sumo.vehicle.getWaitingTime(veh) for veh in vehicles] # [waitv1 waitv2]

        accum_waiting_times = [self.sumo.vehicle.getAccumulatedWaitingTime(veh) for veh in vehicles]

        system_tyre_pm = get_tyre_pm()  # abs accelerations of all vehicles in network 
        system_arrived_num = self.sumo.simulation.getArrivedNumber() 
        
        # append to class cumulative counters - absolute accelerations
        self.episode_sys_abs_accel += system_tyre_pm 
        self.episode_sys_arrived_so_far += system_arrived_num 
        
        system_stats = {
            "system_abs_accel": system_tyre_pm,
            "system_total_stopped": sum(int(speed < 0.1) for speed in speeds),
            "system_total_waiting_time": sum(waiting_times),
            "system_avg_waiting_time": 0.0 if len(vehicles) == 0 else np.mean(waiting_times),
            "system_avg_speed": 0.0 if len(vehicles) == 0 else np.mean(speeds),
            "system_arrived_vehicles_at_destination": system_arrived_num,
            "system_accum_waiting_time" : sum(accum_waiting_times)
        }
        
        return system_stats
    
    def compute_agent_metrics(self):
        '''log agent related metrics - to be used within every SUMO simulation step''' 
        
        stopped = [self.traffic_signals[ts].get_total_queued() for ts in self.ts_ids]
        accumulated_waiting_time = [
            sum(self.traffic_signals[ts].get_accumulated_waiting_time_per_lane()) for ts in self.ts_ids
        ]
        average_speed = [self.traffic_signals[ts].get_average_speed() for ts in self.ts_ids]
        abs_accelerations = [get_tyre_pm(ts) for ts in self.traffic_signals.values()]

        # append to agent-specific cumulative counters 
        for i, ts in enumerate(self.ts_ids):
            self.episode_abs_accel[i] += abs_accelerations[i]
            self.episode_stopped[i] += stopped[i]
            self.episode_avg_speed[i] += average_speed[i]
            self.episode_accumulated_waiting_time[i] += accumulated_waiting_time[i]

            # keep record of total reward 
            if self.rewards[ts] is not None:
                self.episode_reward_total[i] += self.rewards[ts] 

        # store values in dictionaries 
        agent_stats = {}
        for i, ts in enumerate(self.ts_ids):
            agent_stats[f"{ts}_abs_accel"] = abs_accelerations[i]
            agent_stats[f"{ts}_stopped"] = stopped[i]
            agent_stats[f"{ts}_average_speed"] = average_speed[i]
            agent_stats[f"{ts}_accumulated_waiting_time"] = accumulated_waiting_time[i]

        return agent_stats

    def compute_episode_agent_stats(self):
        '''Compute total/average of summary class variables
            Should be called only at the end of an episode or period of infinite horizon'''

        # either returns sumo time period over an episode or
        # returns sumo time period equal to training period 

        assert self.sim_step == self.sim_max_time, \
            f"Simulation step {self.sim_step} does not equal the maximum simulation time {self.sim_max_time}"
        
        time_period = self.sim_max_time
   
        summary_episode_agent_stats = {}
        
        for i, ts in enumerate(self.ts_ids):
            summary_episode_agent_stats[f"{ts}_episode_abs_accel"] = self.episode_abs_accel[i]
            summary_episode_agent_stats[f"{ts}_episode_avg_speed"] = self.episode_avg_speed[i] / time_period
            summary_episode_agent_stats[f"{ts}_episode_accumulated_waiting_time"] = self.episode_accumulated_waiting_time[i] / time_period
            summary_episode_agent_stats[f"{ts}_episode_stopped"] = self.episode_stopped[i] / time_period
            summary_episode_agent_stats[f"{ts}_episode_reward_total"] = self.episode_reward_total[i]
        
        return summary_episode_agent_stats
   
    # def compute_episode_world_stats(self):

    #     # either returns sumo time period over an episode or
    #     # returns sumo time period equal to training period 
    #     assert self.sim_step == self.sim_max_time, \
    #         f"Simulation step {self.sim_step} does not equal the maximum simulation time {self.sim_max_time}"
    #     time_period = self.sim_max_time

    #     summary_episode_world_stats = {}
    #     summary_episode_world_stats[f"episode_sys_abs_accel"] = self.episode_sys_abs_accel / time_period
    #     summary_episode_world_stats[f"episode_sys_arrived_so_far"] = self.episode_sys_arrived_so_far / time_period
        
    #     return summary_episode_world_stats
    
    def reset_episodic_stats(self):
        '''reset the episodic variables'''
        # Initialise cumulative system counters
        self.episode_sys_abs_accel = 0
        self.episode_sys_arrived_so_far = 0

        # Initialise episodic summary counters
        self.episode_abs_accel = [0] * len(self.ts_ids)
        self.episode_avg_speed = [0] * len(self.ts_ids)
        self.episode_accumulated_waiting_time = [0] * len(self.ts_ids)
        self.episode_stopped = [0] * len(self.ts_ids)
        self.episode_reward_total = [0] * len(self.ts_ids)

    def log_csv_step_metrics(self, system_stats, agent_stats, summary_episode_agent_stats):
        '''log metrics to csv file. If data is none, then plot all zeros'''
        
        if self.csv_path:
            with open(self.csv_path, "a", newline="", ) as f:
                csv_writer = csv.writer(f, lineterminator='\n')
                
                if not summary_episode_agent_stats: # if these are Nones 
                    data = ([self.sim_step, self.episode]
                            + list(system_stats.values())
                            + list(agent_stats.values())
                            + [0 for i in range(self.ts_summary_agent_metrics_dims)])
                else: 
                    data = ([self.sim_step, self.episode]
                            + list(system_stats.values())
                            + list(agent_stats.values())
                            + list(summary_episode_agent_stats.values()))
                    
                csv_writer.writerow(data)

    def log_tensor_summary_metrics(self, summary_episode_agent_stats, summary_episode_world_stats, label): 
        '''log the summary metrics at end of every episode. Should be used during training'''
        x = self.episode

        # Log to TensorBoard
        assert (self.sim_step == self.sim_max_time), \
            f"Simulation step {self.sim_step} does not equal the maximum simulation time, {self.sim_max_time}"
        
        # for stat, val in summary_episode_world_stats.items():
        #         self.tb_writer.add_scalar(f"{label}/system/{stat}", val, x)
    
        for stat, val in summary_episode_agent_stats.items():
                self.tb_writer.add_scalar(f"{label}/agent_{stat[0]}/{stat[2:]}", val, x)
            
    def log_tensor_step_metrics(self, agent_stats, system_stats, label):
        '''record metrics for every step in the system. Should be used when running single evaluations.'''
        if hasattr(self, "tb_writer"):
            for stat, val in system_stats.items():
                self.tb_writer.add_scalar(f"{label}/system/{stat}", val, self.sim_step)

            for stat, val in agent_stats.items():
                self.tb_writer.add_scalar(f"{label}/agent_{stat[0]}/{stat[2:]}", val, self.sim_step)

        # self.tb_writer.add_scalar("world/abs_accel_cumulative", self.episode_sys_abs_accel, self.sim_step)
        # self.tb_writer.add_scalar("world/episode_sys_arrived_so_far", self.episode_sys_arrived_so_far, self.sim_step)    
        
    def step(self, action: Union[dict, int]):
        """Apply the action(s) and then step the simulation for delta_time seconds.

        Args:
            action (Union[dict, int]): action(s) to be applied to the environment.
            If single_agent is True, action is an int, otherwise it expects a dict with keys corresponding to traffic signal ids.
        """
        # No action, follow fixed TL defined in self.phases
        if self.fixed_ts or action is None or action == {}:
            # Rewards for the sumo steps between every env step
            self.reward_hold = Counter({ts: 0 for ts in self.ts_ids})
            for _ in range(self.delta_time):
                self._sumo_step()

                r = {ts: self.traffic_signals[ts].compute_reward() for ts in self.ts_ids}
                self.reward_hold.update(r)  # add r to reward_hold Counter
        else:
            self._apply_actions(action)
            self._run_steps()

        observations = self._compute_observations()
        rewards = self._compute_rewards()
        dones = self._compute_dones()
        terminated = False  # there are no 'terminal' states in this environment
        truncated = dones["__all__"]  # episode ends when sim_step >= max_steps
        info = self._compute_info()

        if self.single_agent:
            return observations[self.ts_ids[0]], rewards[self.ts_ids[0]], terminated, truncated, info
        else:
            return observations, rewards, dones, info
    
    def _run_steps(self): # this func is also called in step() function 
        # Rewards for the sumo steps between every env step
        self.reward_hold = Counter({ts: 0 for ts in self.ts_ids})
        time_to_act = False
        while not time_to_act:
            self._sumo_step()
            r = {ts: self.traffic_signals[ts].compute_reward() for ts in self.ts_ids}
            self.reward_hold.update(r)  # add r to reward_hold Counter

            for ts in self.ts_ids:
                self.traffic_signals[ts].update()
                if self.traffic_signals[ts].time_to_act:
                    time_to_act = True

    def _compute_rewards(self): # this func is called in step()
        self.rewards.update(
            {ts: self.reward_hold[ts] for ts in self.ts_ids if self.traffic_signals[ts].time_to_act or self.fixed_ts}
        )
        return {ts: self.rewards[ts] for ts in self.rewards.keys() if self.traffic_signals[ts].time_to_act or self.fixed_ts} # same thing as returning rewards_hold ... why not return rewards_hold?

    def _compute_info(self):
        info = {"__common__": {"step": self.sim_step}}
        per_agent_info = self._get_per_agent_info()

        for agent_id in self.ts_ids:
            agent_info = {}

            for k, v in per_agent_info.items():
                if k.startswith(agent_id):
                    agent_info[k.split("_")[-1]] = v

            # Add tyre PM
            agent_info["tyre_pm"] = get_tyre_pm(self.traffic_signals[agent_id])

            info.update({agent_id: agent_info})

        return info

    def get_observable_vehs(self, lane) -> List[str]:
        """Remove undetectable vehicles from a lane."""
        detectable = []
        for vehicle in self.sumo.lane.getLastStepVehicleIDs(lane): # Returns the ids of the vehicles for the last time step on the given lane
            path = self.sumo.vehicle.getNextTLS(vehicle) # Return list of upcoming traffic lights [(tlsID, tlsIndex, distance, state), ...]
            if len(path) > 0:
                next_light = path[0]
                distance = next_light[2]
                if distance <= self.max_dist:  # Detectors have a max range. if distance from vehicle to traffic light is < 200, vehicle is detectable from traffic light
                    detectable.append(vehicle)
        return detectable
    
    def reset(self, seed: Optional[int] = None, **kwargs):
        """Reset the environment."""
        super().reset(seed=seed, **kwargs)

        # reset cumulative counters
        self.episode_sys_abs_accel = 0
        self.episode_sys_arrived_so_far = 0
        
        # reset episodic summary counters
        self.episode_abs_accel = [0] * len(self.ts_ids)
        self.episode_avg_speed = [0] * len(self.ts_ids)
        self.episode_accumulated_waiting_time = [0] * len(self.ts_ids)
        self.episode_stopped = [0] * len(self.ts_ids)
        self.episode_reward_total = [0] * len(self.ts_ids)

# class same as SumoEnvironmentPZ, (as it inherits from sumoEnvPZ), BUT THIS TIME, 
# instead of self.env = SumoEnvironment(), its self.env = SumoEnvironmentCountAllRewards()
class SumoEnvironmentPZCountAllRewards(SumoEnvironmentPZ):
    """A wrapper for `SumoEnvironmentCountAllRewards` subclasses SumoEnvironmentPZ"""
    def __init__(self, single_eval_mode: bool = False, multiple_eval_mode : bool = False, csv_path: Optional[str] = None, tb_log_dir: Optional[str] = None, **kwargs):
        EzPickle.__init__(self, **kwargs)
        self._kwargs = kwargs

        self.seed()
        # create an instance of the SUMOEnvironment
        self.env = SumoEnvironmentCountAllRewards(**self._kwargs, 
                                                  single_eval_mode=single_eval_mode,
                                                  multiple_eval_mode=multiple_eval_mode,
                                                  csv_path=csv_path,
                                                  tb_log_dir=tb_log_dir)  # instead of SumoEnvironment. CountAllRewardsEnv is subclass of SumoEnv  

        self.agents = self.env.ts_ids  # do you really need to redefine all the attributes, they are already inherited from SumoEnvPz?
        self.possible_agents = self.env.ts_ids
        self._agent_selector = agent_selector(self.agents)
        self.agent_selection = self._agent_selector.reset()
        # spaces
        self.action_spaces = {a: self.env.action_spaces(a) for a in self.agents}
        self.observation_spaces = {a: self.env.observation_spaces(a) for a in self.agents}

        # dicts
        self.rewards = {a: 0 for a in self.agents}
        self.terminations = {a: False for a in self.agents}
        self.truncations = {a: False for a in self.agents}
        self.infos = {a: {} for a in self.agents}

    def step(self, action):
        """Step the environment. Need to override original parent class, 
        reason why is because in the parent class, it counts all rewards within _run_steps()
        function, which is implemented in the SumoEnvCountAllRewards() class.
        However it misses to measure rewards if self.env.fixed_ts is set to True.
        Therefore this class serves to put it here.
        """
        if self.truncations[self.agent_selection] or self.terminations[self.agent_selection]:
            return self._was_dead_step(action)
        agent = self.agent_selection # agent currently being stepped 
        
        # if not self.env.fixed_ts:
        if not self.action_spaces[agent].contains(action):
            raise Exception(
                "Action for agent {} must be in Discrete({})."
                "It is currently {}".format(agent, self.action_spaces[agent].n, action)
            )

        if not self.env.fixed_ts:
            self.env._apply_actions({agent: action})

        if self._agent_selector.is_last(): # check if current agent is last agent 
            if not self.env.fixed_ts:
                self.env._run_steps()
            else:
                # measure rewards 
                self.env.reward_hold = Counter({ts: 0 for ts in self.env.ts_ids})
                for _ in range(self.env.delta_time):
                    self.env._sumo_step()
                    r = {ts: self.env.traffic_signals[ts].compute_reward() for ts in self.env.ts_ids}
                    self.env.reward_hold.update(r)  # add r to reward_hold Counter
            self.env._compute_observations()
            self.rewards = self.env._compute_rewards()
            self.compute_info()
            # print("self.rewards", self.rewards)
        else:
            self._clear_rewards()

        done = self.env._compute_dones()["__all__"]
        self.truncations = {a: done for a in self.agents}

        self.agent_selection = self._agent_selector.next()
        self._cumulative_rewards[agent] = 0
        self._accumulate_rewards()

    # @property
    # def action_space(self):
    #     """Return the biggest action space of the traffic signal agents."""
    #     spaces = [self.action_spaces[ts] for ts in self.env.traffic_signals]
    #     max_n = max([space.n for space in spaces])
    #     return Discrete(max_n)

    # def close(self):
    #     """Close the environment and stop the SUMO simulation."""
    #     if self.env.sumo is None:
    #         return

    #     if not LIBSUMO:
    #         traci.switch(self.env.label)
    #     traci.close()

    #     # Help completely release SUMO port between episodes to address
    #     # "Unable to create listening socket: Address already in use" error
    #     time.sleep(2)

    #     if self.env.disp is not None:
    #         self.env.disp.stop()
    #         self.env.disp = None

    #     self.env.sumo = None